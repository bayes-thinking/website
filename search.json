[
  {
    "objectID": "examples/2-1-probability-in-class-examples.html",
    "href": "examples/2-1-probability-in-class-examples.html",
    "title": "Examples for Review of Basic Probability - Part 2",
    "section": "",
    "text": "Conditional Probability\n\nExample 1Example 2Example 3\n\n\nIn a recent survey, a random sample of adult Americans (18+) was asked, “How likely are you to trust a news report from American media, such as MSNBC or Fox News?”\n[contingency table here]\n\nWhat is the probability that a randomly selected individual is 55+ years of age given that they are more likely to trust the news report?\nWhat is the probability that a randomly selected individual is more likely to trust the news report given that they are 55+ years of age?\n\n\n\n\n\n\nSet up: separate class into groups.\nMammograms are used to detect breast cancer. Suppose a mammogram is known to be 80% accurate. Further, suppose that in the United States, 50 million women are tested for breast cancer with mammograms and, of those tested, 250,000 have breast cancer (regardless of the results of the mammogram). Finally, the probability of a false positive is 0.07.\n\nWhat is the mathematical representation of suppose a mammogram is known to be 80% accurate?\nWhat is the mathematical representation of probability of a false positive is 0.07?\nUsing the information given, construct the contingency table for mammogram \\(\\times\\) cancer diagnosis.\n\n\n\n\n\n\nIndependent Events\n\nExample 1Example 2Example 3\n\n\nThe probability that a randomly selected person in the United States earns more than $100,000 per year is 0.179.\nThe probability that a randomly selected person in the United States earns more than $100,000 per year given that they have earned a bachelor’s degree is 0.371.\nAre these two events independent?\n\n\n\n\n\n\n\n\n\n\n\nMultiplication Rule for Independent Events\n\nExample 1Example 2Example 3"
  },
  {
    "objectID": "examples/5-1-priors-in-class-examples.html",
    "href": "examples/5-1-priors-in-class-examples.html",
    "title": "Examples for Prior Distributions",
    "section": "",
    "text": "library(bayesrules)\nlibrary(tidyverse)"
  },
  {
    "objectID": "examples/5-1-priors-in-class-examples.html#language-notes-about-nomenclature",
    "href": "examples/5-1-priors-in-class-examples.html#language-notes-about-nomenclature",
    "title": "Examples for Prior Distributions",
    "section": "Language / notes about nomenclature",
    "text": "Language / notes about nomenclature"
  },
  {
    "objectID": "examples/5-1-priors-in-class-examples.html#beta-prior",
    "href": "examples/5-1-priors-in-class-examples.html#beta-prior",
    "title": "Examples for Prior Distributions",
    "section": "Beta Prior",
    "text": "Beta Prior\n\nReview the beta distribution\nSuppose we are looking at binary outcomes; we want to put a prior on \\(\\pi = P[Y=1]\\), meaning \\(\\pi \\in [0, 1]\\).\nThe Beta model (often used to describe the variability in \\(\\pi\\)) has shape parameters \\(\\alpha &gt; 0\\) and \\(\\beta &gt; 0\\), and these are the shape hyperparameters.\n\\[\\pi \\sim \\text{Beta}\\left(\\alpha, \\beta \\right),\\]\nThe Beta model’s pdf is\n\\[f\\left( \\pi \\right) = \\frac{\\Gamma \\left( \\alpha + \\beta \\right)}{\\Gamma \\left( \\alpha \\right) \\Gamma \\left( \\beta \\right)} \\pi^{\\alpha-1} (1-\\pi)^{\\beta-1},\\]\n\nNote the following:\n\n\\(\\Gamma\\left( z \\right) = \\int_{0}^{\\infty} x^{z-1} e^{-y} dx\\)\n\\(\\Gamma\\left( z + 1 \\right) = z \\Gamma\\left( z \\right)\\)\nif \\(z\\in \\mathbb{Z}^+\\), then \\(\\Gamma\\left( z \\right) = (z-1)!\\)\n\n\n\nExample 1Example 2Example 3\n\n\nLet’s play with plotting the Beta distribution.\n\nplot_beta(5, 5)\n\n\n\nplot_beta(15, 15)\n\n\n\nplot_beta(30, 30)\n\n\n\nplot_beta(2, 5)\n\n\n\nplot_beta(5, 2)\n\n\n\nplot_beta(2,1)\n\n\n\n\n\n\nJillian is a soccer player who, throughout her career in competitive soccer, has probability 0.5 of scoring when she attempts. However, her coach suspects that she’s getting better. Her coach begins keeping tabs and is going to ask us to analyze the data. While we wait for them to complete data collection, we can go ahead and decide on the prior distribution.\nThinking about our original exploration of the Beta distribution,\n\nplot_beta(5, 5)\n\n\n\nplot_beta(15, 15)\n\n\n\nplot_beta(30, 30)\n\n\n\n\nThe above distributions are centered at 0.5, we just need to decide on which is best for our analysis. What should we choose?\n\n\nSuppose we are estimating the population proportion of children with vanishing white matter disease, a rare disease. What should our prior be?"
  },
  {
    "objectID": "examples/5-1-priors-in-class-examples.html#normal-prior",
    "href": "examples/5-1-priors-in-class-examples.html#normal-prior",
    "title": "Examples for Prior Distributions",
    "section": "Normal Prior",
    "text": "Normal Prior\n\nReview the normal distribution\nSuppose we are now examining a continuous outcome. Let \\(Y\\) be a continuous random variable that can take any value in \\(\\mathbb{R}\\); i.e., \\(Y \\in \\left(-\\infty, \\infty\\right)\\).\nLet us assume that the variability in \\(Y\\) can be represented by the normal distribution with mean parameter \\(\\mu \\in \\mathbb{R}\\) and standard deviation parameter \\(\\sigma \\in \\mathbb{R}^+\\).\n\\[Y \\sim N\\left(\\mu, \\sigma^2\\right)\\]\nThe normal model’s pdf is\n\\[f(y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left\\{-\\frac{\\left(y - \\mu\\right)^2}{2\\sigma^2} \\right\\}\\]\n\nNote: \\(\\sigma\\) provides a sense of scale for \\(Y\\); approximately 95% of \\(Y\\) values will be within 2 standard deviations.\n\ni.e., \\(\\mu \\pm 2 \\sigma\\)\n\n\n\nExample 1Example 2Example 3\n\n\nWhat happens as variability increases?\n\nplot_normal(0, 1) + ylim(0,0.5)\n\n\n\nplot_normal(0, 10) + ylim(0,0.5)\n\n\n\nplot_normal(0, 100) + ylim(0,0.5)\n\n\n\n\nWhat happens as variability decreases?\n\nplot_normal(0, 1) + ylim(0, 1)\n\n\n\nplot_normal(0, 0.75) + ylim(0, 1)\n\n\n\nplot_normal(0, 0.5) + ylim(0, 1)\n\n\n\n\n\n\nA new Introduction to Biostatistics instructor believes that exam grades should follow a normal distribution. Construct the following priors; remember to keep the y-axis on the same scale for all graphs.\nA prior with \\(\\mu=75\\) and high variability:\nA prior with \\(\\mu=75\\) and medium variability:\nA prior with \\(\\mu=75\\) and low variability:\n\n\nLet \\(\\mu\\) be the average 5 p.m. temperature in Pensacola. Dr. Seals believes that Pensacola is hot year round, so she believes that the average temperature is probably around 85 degrees Fahrenheit. However, you are skeptical that it is hot all year, so let’s come up with a prior for this."
  },
  {
    "objectID": "lectures.html",
    "href": "lectures.html",
    "title": "Bayesian Thinking",
    "section": "",
    "text": "Topic\nNotes\nExamples\n\n\n\n\nIntroduction to Technology\n\n\n\n\nProbability\n\n\n\n\nProbability Distributions\n\n\n\n\nBayes Theorem\n\n\n\n\nDistributions: Priors\n\n\n\n\nDistributions: Data\n\n\n\n\nBeta-Binomial\n\n\n\n\nNormal-Normal\n\n\n\n\nMCMC & Gibbs Sampler\n\n\n\n\nPredictions"
  },
  {
    "objectID": "instructors/6-0-likelihood.html",
    "href": "instructors/6-0-likelihood.html",
    "title": "Likelihood - Data Distributions",
    "section": "",
    "text": "Lesson Plan: Likelihood - Data Distributions\nObjectives - Students will be able to identify the proper distribution that would be used to model various scenarios.\n- Students will calculate probabilities and likelihoods.\nDuration: 75 minutes\nMaterials:\nIntroduction Present a situation modeled by a discrete distribution and a situation modeled by a continuous situation. Have some groups of students discuss which situation, A or B, could be modeled with a discrete distribution. Have the other groups of students discuss which situation, A or B, could be modeled with a continuous distribution. All groups should be able to justify their choice with a valid mathematical reason."
  },
  {
    "objectID": "instructors/6-0-likelihood.html#main-content",
    "href": "instructors/6-0-likelihood.html#main-content",
    "title": "Likelihood - Data Distributions",
    "section": "Main Content:",
    "text": "Main Content:\nLikelihood- When your data x is known, the likelihood function L(π| x) allows us to compare the likelihoods of different scenarios for the parameter value π, producing data x.\n-The likelihood function provides the tool we need to evaluate the relative compatibility of different values for parameter π with data x.\n-When working with data, identifying the distribution that would best model your data is useful when working to create a posterior distribution from a prior distribution and your data (likelihood function).\n-Today, we will review (this is meant to be a brief review so that you can have the students complete the activity.) some of the most useful and common distributions we will use in this course. One of our goals today is to be able to identify scenarios that these distributions would model.\n-We will first look at discrete distributions and then at continuous distributions.\n\nDiscrete Distributions\n\nBinomialPoisson\n\n\n\nBinomial(n,π) is used to model the number of successes in a sample of size n when drawn with replacement from a population of size N.\n\nWhere n is the number of independent trials, and\nπ is the probability of success for each trial.\n\n\nThe following properties of the binomial distribution may help you identify when a situation should be modeled this way.\nProperties of the Binomial Distribution\n\nTwo possible outcomes: success or failure, have a characteristic or do not have a characteristic.\nn number of independent trials or a fixed number of n times repeated trials\nProbability of success is the same for each trial\n\nExample-Identifying Distribution: When modeling side effects from a new drug, a researcher knows that 10% of the population experience a side effect when taking this class of drugs. The researcher has a sample of 1,800 people. The researcher wants to know the probability that less than 50 people from the sample will experience a side effect. The distribution would be Binomial(800, .10)\n\n\n\nPois(λ) The Poisson models the probability of an event happening a certain number of times within a given interval of time or space.\nThe events occur independently with a constant mean rate, λ, which is the mean number of events in a fixed interval.\n\nExample: JCPenney’s call center expects 100 calls per hour. What is the probability that the center will receive 78 calls in the next hour? The distribution we could use to model this situation is Poisson, *Pois(100)**.\n\n\n\n\n\nContinuous Distributions\n\nNormal - N(µ,σ)Uniform - U(a, b)Beta - Beta (α, β)GammaExponential - exp(λ)\n\n\nProperties of the Normal Distribution:\n\nBell-shaped, symmetric, and unimodal.\nRoughly follows the empirical rule\n-About 68% of data falls within one standard deviation of the mean.\n-About 95% of data falls within two standard deviations of the mean, and\n-About 99.7% of data falls within three standard deviations of the mean.\n\nThis distribution is used to model many situations.\nAsk students:\n\nShare an example of a situation that they have modeled with a normal distribution with their group.\nThen, have the groups share some of their examples.\n\nRecord students’ examples in the notes.\n\n\nThe uniform distribution is a probability distribution in which every value between an interval [a, b] is equally likely to occur. Ask students:\n\nShare an example of when we would use a uniform distribution.\n\nRecord examples in the notes.\n\n\nUnlike other distributions with shape and scale parameters, the beta distribution has two shape parameters, α and β.\n\nBoth parameters must be positive values.\nThe Beta distribution is a probability distribution on probabilities, so its domain is bounded between 0 and 1.\nThe beta distribution represents all of the possible values of a probability when we don’t know that probability.\n\nWe could use a Beta distribution to model the following situation:\nScenario: A drug company believes their new medicine will be effective on humans 90% of the time. The medicine is tried on 200 patients, and it is effective with 180 patients.This is a Beta(90, 10).\nTeacher’s Notes:\nDesmos has the beta function parameterized in a different format, but it is easy to alter. https://www.desmos.com/calculator/kx83qio7yl\nConsider using this video either in class or as an assignment. The video is 16 minutes long and relates the beta distribution to data science and Bayesian statistics. This might be better for a graduate course. https://www.youtube.com/watch?v=1k8lF3BriXM\n\n\nThe gamma distribution uses two parameters. There are two equivalent parameterizations, and the second one listed below is more commonly used in Bayesian statistics.\n\nΓ(k, ϴ) Shape parameter k and scale parameter ϴ.\nΓ(α, β) Shape parameter α = k and an inverse scale parameter β = 1/θ\n\n\nThe gamma distribution describes the waiting time until a certain number of events occur in a Poisson process with a given rate.\nIt also is used to model variables that are positive and right-skewed.\n\nSpecial cases of the gamma distribution are the exponential, Erlang, and Chi-Squared.\n\nExample: Amazon’s call center receives calls at an average rate of β = 500 calls per hour. The time until the kth call will have the distribution Γ(k, 1/500), since the scale parameter is ϴ =1/500\n\n\nThe exponential probability distribution is used to model the time we must wait until a certain event occurs.\nThe rate parameter, λ (calculated as λ = 1/μ).\nExample Modeling the time a postal clerk spends with their customers. In this case, let’s say it is 2 minutes, then that would be Exp (1/2).\nConfused about the difference between Poisson and exponential distribution?\n\nRemember that the Poisson distribution is discrete and deals with the number of occurrences of events in a fixed period of time.\nExponential distribution is continuous and is often concerned about the amount of time until an event happens.\n\nExample: A new customer enters the post office every two minutes, on average. After a customer arrives, find the probability that a new customer will arrive in less than one minute.\nThe average time between customers is two minutes. Thus, the rate can be calculated as: λ = 1/μ = ½=.5\nSolution: To answer this, we will need to use the CDF of the exponential distribution.\nWe can substitute in λ = 0.5 and x = 1 to the formula for the CDF:\n\n  P(X ≤ x) = 1 – e-λx\n  P(X ≤ 1) = 1 – e-0.5(1)\nP(X ≤ 1) = 0.3935\n\nThe probability that we’ll have to wait less than one minute for the next customer to arrive is 0.3935.\nReview question for Students that you may ask in class or give for homework: How could you do this calculation using R?\n\n\n\n\n\nActivities\n\nActivity 1Activity 2\n\n\nClass Activity - Recognition of Data Distributions\nCreate a sort. Mix up the scenarios and have them match to the correct distribution. You could print the scenarios on index cards and have them organize them that way or use Desmos to create an electronic version of the card sort.\nBinomial distribution (discrete)\n\nSituation: Neilson survey for watching or not watching a TV show.\nSituation: Model the probability that a certain number of credit card transactions are fraudulent per day.\nSituation: Modeling the number of returns to a store per week based on the number of transactions.\n\nPoisson distribution (discrete)\n\nSituation: A manager of Panda Express wants to model the number of expected customers that will arrive at the restaurant per day.\nSituation: Model the number of expected visitors per hour that a website receives so that there is enough bandwidth to handle a specific number of visitors.\nSituation: Model the number of power grid failures in a week.\n\nNormal distribution (continuous)\n\nSituation: A hospital wants to model the birth weight of newborn babies in Virginia to identify babies born with unusually low birth weights.\nSituation: A pediatrician wants to model a boy’s height at age 16 to identify if a boy is unusually tall as a sign of Marfan syndrome.\nSituation: A financial advisor wants to model the average retirement age of an NFL player so that they can present several investing strategies to players from the Pittsburgh Steelers.\n\nUniform distribution (continuous)\n\nSituation: You want to model the probability of people having a specific birthday date.\nSituation: A gambler would like to know the probability of rolling a 1-6 with a six-sided dice.\nSituation: The Virginia Lottery designs a new scratcher game and prints a million of them, and only 1 is a winner. They need to know the probability of each card printed being a winner.\n\nBeta distribution (continuous)\nThis is a versatile distribution so students find this difficult to identify.\n\nSituation: A social media influencer wants to model the likelihood of people clicking the follow button.\nSituation: A politician wants to model the probability of the incumbent winning a second term in office.\nSituation: A researcher wants to model the 5-year survival rate of men with prostate cancer using radiation only as a treatment.\n\nGamma distribution (continuous)\n\nSituation: JCPenney’s call center manager wants to be able to model the time until a particular number of calls have come into their center.\nSituation: A manager of Bernetti’s Pizza wants to model the time until the 50th order is called into the restaurant so that he can estimate when their business is in the profit zone.\nSituation: A bank manager knows that, on average, there are 100 customers that are served. He would like to know the time it takes for 80 of the customers to be served.\nExponential distribution (continuous)\nSituation: How long will my new dishwasher will work before it breaks down?\nSituation: How long will it be until Virginia has another 5.8 or greater magnitude earthquake? (The last one was August 23, 2011, in Louisa, Virginia)\nSituation: How long does your professor have to wait until a student enters the classroom?\n\nNow that we have a better idea of what distribution to use to model these different situations, we are going to look at how data is used in Bayesian analysis.\n\n\nAnalysis- Is this Fake News?: Exclamation Point in Title Helps with Identification\nThis activity utilizes Chapter 2 from Bayes Rules!** (Johnson et al.,2021).\nExamine a sample of 150 articles that were posted on Facebook and fact-checked by five BuzzFeed journalists (Shu et al. 2017). Information about each article is stored in the fake_news dataset in the bayesrules package. To learn more about this dataset,type ?fake_news in your console.\n\n# Load packages\nlibrary(bayesrules)\nlibrary(tidyverse)\nlibrary(janitor)\n\n# Import article data\ndata(fake_news)\n\nUsing the janitor package, we can create a table which helps us see the type of article, how many n, and the percent. Use the top 3 lines to get the information in the table below those lines.\n\nfake_news %&gt;% \n  tabyl(type) %&gt;% \n  adorn_totals(\"row\")\n\n\n\n\nTable: Prior\n\n\nThis data provides us with information to begin our analysis and we call this the prior distribution.\nNext, we want to see how many real and fake news articles use exclamation points in the article title.\nCopy the first 4 rows into your console to obtain the table below those rows.\n\n# Tabulate exclamation usage and article type\nfake_news %&gt;% \n  tabyl(title_has_excl, type) %&gt;% \n  adorn_totals(\"row\")\n\nBelow, you will see a completed two-way table. The variable in the columns tells us the type of news the article was classified, fake or real. The variable in the rows tells us if the title of the article has an exclamation point, is true, or is not false.\n\n\n\nTable: data\n\n\nCalculation of the Likelihoods\n\nWhat is the probability that the title has an exclamation point given that the article is fake news? likelihood: P(True|Fake)=16/60 = .2667, about 26.67%\nWhat is the probability that the title has an exclamation point given that the article is real news? likelihood: P(True|Real)=2/90=.0222, about a 2.22%\n\n\n\n\nTable: Prior and Likelihood (data)\n\n\nNote: The likelihoods do not have to add to 1.\nA joint probability model of the fake status and exclamation point usage across all articles can be calculated.\n\n\n\nTable: Joint Probability Model\n\n\n\nP(True|Fake) = 0.2667 x 0.4 = 0.1067\nP(False|Fake) = [(1-.2667)x.4] = [.7333x.4] = 0.2933\nP(True|Real) = 0.0222 x 0.6 = 0.0133\nP(False|Real) = [(1-.0222)x.6] = [.9778x.6] = 0.5867\n\nKnowing the likelihood distribution can help us pair it with a conjugate prior (which is something we will be learning about in a future class). When we have a conjugate prior, then we have an easy-to-derive and interpret the posterior model.\nHere is a listing that may assist you in the pairing of these two distributions: https://en.wikipedia.org/wiki/Conjugate_prior"
  },
  {
    "objectID": "instructors/6-0-likelihood.html#conclusion",
    "href": "instructors/6-0-likelihood.html#conclusion",
    "title": "Likelihood - Data Distributions",
    "section": "Conclusion:",
    "text": "Conclusion:\n\nAsk students summarize what they learned today.\n\nMake sure to have students re-voicing what other students are saying. (This will encourage engagement.)\n\nAsk students to explain what one of their peers said.\nProvide each student with an Exit Ticket and have that include some questions that check for understanding.\nExample Questions for Exit Ticket:\n\nYou may ask students to tell you what they are confident they understand.\nHve students list a topic or concept they are still grappling with to understand. (This will help you plan for any necessary review at the beginning of the next class.)"
  },
  {
    "objectID": "instructors/6-0-likelihood.html#assignment",
    "href": "instructors/6-0-likelihood.html#assignment",
    "title": "Likelihood - Data Distributions",
    "section": "Assignment:",
    "text": "Assignment:\nWork through Chapter 7 in the Probability and Bayesian Modeling by Albert & Hu. This takes you through a similar problem, showing you how to use R to assist with the calculations. https://monika76five.github.io/ProbBayes/"
  },
  {
    "objectID": "instructors/6-0-likelihood.html#references",
    "href": "instructors/6-0-likelihood.html#references",
    "title": "Likelihood - Data Distributions",
    "section": "References:",
    "text": "References:\nAlbert, J., & Hu, J. (2019). Probability and Bayesian modeling. CRC press. https://monika76five.github.io/ProbBayes/\nConjugate prior. (2023, July 30). In Wikipedia. https://en.wikipedia.org/wiki/Conjugate_prior\nFirke, Sam. 2021. Janitor: Simple Tools for Examining and Cleaning Dirty Data. https://github.com/sfirke/janitor.\nJohnson, A. A., Ott, M. Q., & Dogucu, M. (2022). Bayes rules!: An introduction to applied Bayesian modeling. CRC Press. https://www.bayesrulesbook.com/foreword\nShu, Kai, Amy Sliva, Suhang Wang, Jiliang Tang, and Huan Liu. 2017. “Fake News Detection on Social Media: A Data Mining Perspective.” ACM SIGKDD Explorations Newsletter 19 (1): 22–36.\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York."
  },
  {
    "objectID": "instructors/8-0-normal-normal-lesson-plan.html",
    "href": "instructors/8-0-normal-normal-lesson-plan.html",
    "title": "Normal-Normal Model",
    "section": "",
    "text": "Objective:\n\nBy the end of this lesson, students will have reviewed the following topics:\n\nWhen to implement the normal-normal model\nHow to implement the normal-normal model\nHow to disseminate analysis results\n\n\nDuration:\n\n75 minutes\n\nMaterials:\n\nHandouts with exercises and problems related to basic probability\nComputer, projector, and screen\n\nIntroduction:\nIntroduce the lesson’s topic:\nMain Content:\n\nThe Normal Model\nRecall the normal model,\n\\[f(y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left\\{-\\frac{\\left(y - \\mu\\right)^2}{2\\sigma^2} \\right\\}\\]\nwhere \\(y\\) is a continuous random variable that can take any value in \\(\\mathbb{R}\\); i.e., \\(Y \\in \\left(-\\infty, \\infty\\right)\\).\nSuppose we have \\(n\\) observations, then the joint distribution is given by\n\\[f(\\overset{\\to}{y}|\\mu) = \\prod_{i=1}^n f(y_i|\\mu) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left\\{-\\frac{\\left(y_i - \\mu\\right)^2}{2\\sigma^2} \\right\\}\\]\nThen the likelihood is given by,\n\\[\n\\begin{align*}\nL(\\mu| \\overset{\\to}{y}) &\\propto \\prod_{i=1}^n \\exp\\left\\{-\\frac{\\left(y_i - \\mu\\right)^2}{2\\sigma^2} \\right\\} = \\exp\\left\\{-\\frac{\\sum_{i=1}^n \\left(y_i - \\mu\\right)^2}{2\\sigma^2} \\right\\} \\\\\n&\\propto \\exp\\left\\{-\\frac{\\left(\\bar{y} - \\mu\\right)^2}{2\\sigma^2/n} \\right\\}\n\\end{align*}\n\\]\n\n\nThe Normal-Normal Model\n\nLet \\(\\mu \\in (-\\infty, \\infty)\\) be an unknown mean parameter and \\((Y_1, Y_2, ..., Y_n)\\) be an independent \\(N(\\mu, \\sigma^2)\\), where \\(\\sigma\\) is assumed to be known.\nThe Normal-Normal Bayesian model has Normal distributions for both prior and data. The Normal prior is on the unknown mean, \\(\\mu\\).\n\n\\[Y_i | \\mu \\overset{\\text{ind}}{\\sim} N(\\mu, \\sigma^2)\\]\n\\[\\mu \\sim N(\\theta, \\tau^2)\\]\n\nWhen we have data \\(\\overset{\\to}{y} = (y_1, ..., y_n)\\) with mean \\(\\bar{y}\\), the posterior distribution for \\(\\mu\\) is also Normal with updated parameters,\n\n\\[\\mu|\\overset{\\to}{y} \\sim N\\left( \\theta \\frac{\\sigma^2}{n\\tau^2 + \\sigma^2} + \\bar{y} \\frac{n\\tau^2}{n\\tau^2+\\sigma^2}, \\frac{\\tau^2 \\sigma^2}{n\\tau^2 + \\sigma^2} \\right)\\]\nCalculation and Practice:\n\nExample 1: concussions\nExample 2: stock prices\n\nDiscussion and Wrap-Up:\n\nFacilitate a class discussion to review the example problems, reinforce key concepts, and answer any questions the students have.\n\nHomework:\n\nAssign additional problems to practice the basic probability rules.\n\nFormative Assessment:\n\nEvaluate students based on their participation in discussions, their ability to solve example problems, and their performance on the assigned homework.\n\nConclusion:\n\nOur goal is to analyze in the best way possible."
  },
  {
    "objectID": "instructors/7-0-beta-binomial-lesson-plan.html",
    "href": "instructors/7-0-beta-binomial-lesson-plan.html",
    "title": "Introduction to the Beta-Binomial Model",
    "section": "",
    "text": "Duration: 75 minutes\nLearning Objectives:\n\nUnderstand the Beta-Binomial distribution and its application in Bayesian modeling.\nImplement the Beta-Binomial model in R.\nInterpret the results of the model.\nGain hands-on experience through practical examples.\n\nMaterials:\n\nR and RStudio installed on participants’ computers.\nSample dataset (e.g., data on the number of successes and total trials for a series of events).\n\nLesson Outline:\nIntroduction (5 minutes)\n\nBriefly recap Bayesian statistics and its advantages.\nIntroduce the Beta-Binomial distribution and its significance in Bayesian modeling.\n\nTheory and Concepts (15 minutes)\n\nExplain the Beta distribution and its parameters (α and β).\nDescribe how the Beta distribution is used as a prior in Bayesian modeling.\nExplain the Binomial distribution and its parameters (n and p).\n\nPractical Implementation (15 minutes)\n\nLoad necessary libraries (e.g., rstan, ggplot2, dplyr).\nGenerate a sample dataset (e.g., a hypothetical dataset of success/failure trials).\n\n\n# Sample R Code\n# Load libraries\n# library(rstan)\n# library(ggplot2)\n# library(dplyr)\n# Generate a sample dataset\n# set.seed(123)\n# successes &lt;- c(12, 15, 17, 19, 20)\n# trials &lt;- c(20, 20, 20, 20, 20)\n# data &lt;- data.frame(successes, trials)\n\n\nDefine the Beta-Binomial model in Stan, specifying the likelihood and prior distributions.\n\n\n# Beta-Binomial model in Stan\n\n#data {\n  #int&lt;lower=0&gt; N;        # Number of observations\n  #int&lt;lower=0&gt; y[N];     # Count data\n  #int&lt;lower=0&gt; n_trials; # Number of trials (constant for a binomial distribution)\n#}\n\n# parameters {\n   #real&lt;lower=0, upper=1&gt; theta;  # Probability parameter\n   #real&lt;lower=0&gt; alpha;           # Beta distribution shape parameter\n   #real&lt;lower=0&gt; beta;            # Beta distribution shape parameter\n#}\n\n#model {\n  # Prior distribution\n # theta ~ beta(alpha, beta);\n\n  # Likelihood\n # y ~ binomial(n_trials, theta);\n#}\n\n\nCompile and fit the model using the rstan package.\n\n\n# Install and load the rstan package\n#install.packages(\"rstan\")\n#library(rstan)\n\n# Define the data\n#data_list &lt;- list(\n  #N = length(your_data),\n  #y = your_data,\n  #n_trials = number_of_trials\n#)\n\n# Compile the Stan model\n#stan_model &lt;- stan_model(file = \"path/to/your/stan/model/file.stan\")\n\n# Fit the model\n#fit &lt;- sampling(stan_model, data = data_list, chains = 4, iter = 2000, warmup = 1000)\n\n# Print the summary of the posterior distribution\n# print(fit)\n\nInterpretation of Results (10 minutes)\n\nExplain how to interpret the results of the Beta-Binomial model, including posterior distributions for parameters like theta (probability of success).\nVisualize the posterior distribution using plots.\n\n\n# Sample R Code:\n# Plot posterior distribution of theta\n# posterior_samples &lt;- as.data.frame(fit)\n# ggplot(posterior_samples, aes(x = theta)) +\n# geom_histogram(binwidth = 0.01, fill = \"blue\", color = \"black\") +\n# labs(title = \"Posterior Distribution of Theta\", x = \"Theta (Probability of Success)\") \n\nHands-on Practice (15 minutes)\n\nProvide additional datasets for participants to analyze using the Beta-Binomial model.\nEncourage participants to fit the model, interpret results, and visualize the posterior distribution.\n\nQ&A and Discussion (10 minutes)\n\nAddress any questions or concerns from participants.\nDiscuss real-world applications and extensions of the Beta-Binomial model.\n\nConclusion (5 minutes)\n\nSummarize key takeaways from the lesson.\nProvide resources and references for further learning.\nEncourage participants to explore more complex Bayesian models and applications.\n\nHomework Assignment: Ask participants to analyze a real-world dataset using the Beta-Binomial model in R and share their findings in the next session.\nAssessment: Evaluate participants’ understanding through their ability to implement the Beta-Binomial model, interpret results, and engage in discussions during the lesson.\n\nAdditional Notes and Resources\nExplanation of the Beta Binomial Bayesian Model with examples from the Health Sciences\nThe Beta-Binomial Bayesian model is a statistical model used in various fields, including health sciences, to analyze and make inferences about data characterized by the number of successes in a fixed number of trials, where the probability of success can vary. It combines two key distributions: the Beta distribution as a prior for the probability of success and the Binomial distribution to model the observed data. This model is particularly useful in health sciences for a wide range of applications, such as clinical trials, epidemiology, and medical testing. Let’s explore the model with a couple of health science examples:\nSome technical notes:\n\\[\nY|\\pi \\sim Bin(n, \\pi)\n\\]\n\\[\n\\pi \\sim Beta(\\alpha, \\beta)\n\\]\n\\[\n\\pi |(Y=y) \\sim Beta (\\alpha + y, \\  \\beta +n -y)\n\\]\n\nPrior Model\n\\[\nf(\\pi)=\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\pi^{\\alpha - 1}\n(1-\\alpha)^{\\beta -1}\\]\nData Model\n\\[\nBin(n, \\pi)\n\\]\nLikelihood function\n\\[\nL(\\pi |y)=\\binom {n}{y}\\pi^y\n(1-\\pi)^{n-y}\\]\nPosterior Model\n\\[\nf(\\pi |y) \\propto f(\\pi)L(\\pi|y) = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\pi^{\\alpha-1}(1-\\pi)^{\\beta-1}\\binom{n}{y} \\propto \\pi^{(y+\\alpha)-1}(1-\\pi)^{(\\beta +n-y)-1} = Beta(\\alpha +y, \\ \\beta+n-y)\n\\]\n\n\nExample 1Example 2\n\n\nSuppose a pharmaceutical company is conducting a clinical trial to evaluate the success rate of a new drug in treating a specific disease. They enroll 100 patients and record how many of them respond positively to the treatment (successes). In this case, the Beta-Binomial model can be used to estimate the probability of success and its uncertainty.\n\nBeta Distribution as Prior: The company might have prior beliefs about the success rate of similar drugs and express it using a Beta distribution. For example, they believe that the success rate follows a Beta(10, 10) distribution, which represents a relatively neutral prior with a mean of 0.5.\n\n\n\nBinomial Distribution for Data: If, out of 100 patients, 60 respond positively to the new drug, the observed data follows a Binomial distribution with parameters n=100 (number of trials) and p (the success rate).\n\nIn this scenario, the Bayesian analysis will update the prior Beta distribution with the observed data, yielding a posterior distribution for the success rate, which provides not only a point estimate but also a credible interval representing the uncertainty around the estimate.\n\n\nEpidemiologists want to estimate the prevalence of a rare disease in a population. They decide to conduct a study in which they take a random sample of 500 individuals and test them for the presence of the disease. In this case, the Beta-Binomial model can be used to estimate the prevalence of the disease in the population.\n\nBeta Distribution as Prior: Epidemiologists might use a Beta(2, 8) distribution as a prior for the prevalence. This implies they have prior knowledge suggesting that the prevalence is low (mean of 0.2) but with some uncertainty.\n\n\n\nBinomial Distribution for Data: If 10 individuals in the sample test positive for the disease, the observed data follows a Binomial distribution with n=500 (number of trials) and p (prevalence).\n\nIn this scenario, the Bayesian analysis will update the prior Beta distribution with the observed data, resulting in a posterior distribution for the prevalence of the disease. This posterior distribution gives a range of credible estimates for the disease prevalence in the population.\n\n\n\nIn both examples, the Beta-Binomial model not only provides a point estimate of the parameter of interest (success rate or disease prevalence) but also quantifies the uncertainty around the estimate, which is a valuable feature in health sciences where decisions can have critical consequences. It allows researchers to make informed decisions, taking both prior beliefs and observed data into account."
  },
  {
    "objectID": "instructors/2-1-probability-2-in-class-examples.html",
    "href": "instructors/2-1-probability-2-in-class-examples.html",
    "title": "Examples for Review of Basic Probability - Part 2",
    "section": "",
    "text": "Conditional Probability\n\nExample 1Example 2Example 3\n\n\nIn a recent survey, a random sample of adult Americans (18+) was asked, “How likely are you to trust a news report from American media, such as MSNBC or Fox News?”\n[contingency table here]\n\nWhat is the probability that a randomly selected individual is 55+ years of age given that they are more likely to trust the news report?\nWhat is the probability that a randomly selected individual is more likely to trust the news report given that they are 55+ years of age?\n\n\n\n\n\n\nSet up: separate class into groups.\nMammograms are used to detect breast cancer. Suppose a mammogram is known to be 80% accurate. Further, suppose that in the United States, 50 million women are tested for breast cancer with mammograms and, of those tested, 250,000 have breast cancer (regardless of the results of the mammogram). Finally, the probability of a false positive is 0.07.\n\nWhat is the mathematical representation of suppose a mammogram is known to be 80% accurate?\nWhat is the mathematical representation of probability of a false positive is 0.07?\nUsing the information given, construct the contingency table for mammogram \\(\\times\\) cancer diagnosis.\n\n\n\n\n\n\nIndependent Events\n\nExample 1Example 2Example 3\n\n\nThe probability that a randomly selected person in the United States earns more than $100,000 per year is 0.179.\nThe probability that a randomly selected person in the United States earns more than $100,000 per year given that they have earned a bachelor’s degree is 0.371.\nAre these two events independent?\n\n\n\n\n\n\n\n\n\n\n\nMultiplication Rule for Independent Events\n\nExample 1Example 2Example 3"
  },
  {
    "objectID": "instructors/11-posterior-prediction-lesson-plan.html",
    "href": "instructors/11-posterior-prediction-lesson-plan.html",
    "title": "Prediction using Posterior",
    "section": "",
    "text": "Objectives:\n\nUnderstand and implement the concept of posterior predictive power in Bayesian Statistics using Markov Chain values.\nLearn to use R and relevant libraries (e.g., Stan, bayesian and ggplot2) to approximate posterior predictive power.\nApply the concept to real-world examples from the medical field.\n\nMaterials Needed:\n\nRStudio or any R environment\nR packages: rstan, bayesian, ggplot2\nExample medical datasets (e.g., clinical trial results, patient outcomes)\n\nIntroduction:\n\nRecap Bayesian Statistics basics, emphasizing posterior distribution.\nIntroduce the concept of posterior predictive power and its importance in model assessment.\nBriefly explain Markov Chain values and their role in Bayesian modeling.\n\nMarkov Chain Monte Carlo (MCMC) and Bayesian Modeling:\n\nProvide an overview of Markov Chain Monte Carlo methods.\nDiscuss Bayesian modeling using R, highlighting the rstan package.\nDemonstrate the process of obtaining samples from the posterior distribution.\n\nPosterior Predictive Power:\n\nDefine posterior predictive power and distinguish between prior and posterior predictive checks.\nEmphasize the need for assessing the model’s ability to generate new data.\nDiscuss the limitations of relying solely on parameter estimates.\n\nUsing Markov Chain Values:\n\nWalk through the steps of using Markov Chain values to approximate posterior predictive power.\nProvide R code snippets for extracting Markov Chain values using rstan.\nIllustrate the importance of visualizing the posterior predictive distribution.\n\n\n# Example R code for using Markov Chain values to approximate posterior predictive power\n\n# Install and load necessary packages\n# install.packages(c(\"rstan\", \"bayesian\", \"ggplot2\"))\n# library(rstan)\n# library(bayesian)\n# library(ggplot2)\n\n# Load your medical dataset (replace \"your_data.csv\" with the actual file name)\n# data &lt;- read.csv(\"your_data.csv\")\n\n# Define the Bayesian model using Stan\n# stan_code &lt;- \"\n# data {\n# int&lt;lower=0&gt; N;\n  # Include your data variables here\n  # Example: real y[N];\n# }\n\n# parameters {\n  # Include your model parameters here\n  # Example: real mu;\n# }\n\n# model {\n  # Include your prior and likelihood specifications here\n  # Example: y ~ normal(mu, sigma);\n# }\n\n# generated quantities {\n # Simulate new data points using posterior samples\n # Example: real y_pred[N];\n # for (i in 1:N) {\n # y_pred[i] = normal_rng(mu, sigma);\n # }\n# }\n\n\n# Compile the Stan model\n# stan_model &lt;- stan_model(model_code = stan_code)\n\n# Fit the model to your data\n# stan_fit &lt;- sampling(stan_model, data = list(N = nrow(data), y = data$your_variable))\n\n# Extract Markov Chain values\n# chain_values &lt;- as.matrix(stan_fit)\n\n# Visualize posterior predictive distribution\n# posterior_predictive &lt;- bayesian::extract_posterior_predictive(stan_fit)\n# ggplot(data, aes(x = your_variable)) +\n#  geom_density() +\n#  geom_density(data = posterior_predictive, aes(y = ..scaled..), color = \"blue\", alpha = 0.5) +\n# labs(title = \"Posterior Predictive Distribution\", x = \"Your Variable\")\n\nExample from the Medical Field:\n\nIntroduce a medical dataset and guide participants through the process of building a Bayesian model.\nDiscuss model assessment and emphasize the use of Markov Chain values.\nProvide hands-on exercises for participants to implement the concepts using their own medical datasets.\n\nHands-On Exercise:\n\nStudents will work on a provided medical dataset or bring their own.\nInstructor will guide students through building a Bayesian model, obtaining Markov Chain values, and assessing posterior predictive power.\nEncourage discussion and troubleshooting.\n\nConclusion:\n\nRecap key points about Bayesian modeling, posterior predictive power, and the use of Markov Chain values in R.\nDiscuss the relevance and applications of these techniques in the medical field.\nEncourage participants to explore additional resources for further learning.\n\nAssessment:\n\nEvaluate students’ understanding through class participation, engagement in the hands-on exercise, and the quality of their interpretations of posterior predictive power in the medical context.\n\n\nAdditional Notes\nPosterior Predictive Power\nPosterior predictive power is a concept in Bayesian statistics that involves assessing the ability of a statistical model to generate new data that is consistent with observed data. It is a crucial aspect of model assessment in Bayesian statistics, as it goes beyond simply fitting a model to observed data and focuses on the model’s ability to make predictions for future or unseen data.\nHere’s a breakdown of the key components:\nPosterior Predictive Distribution:\nIn Bayesian statistics, the posterior predictive distribution is obtained by combining the likelihood function (which describes how well the model explains the observed data) with the posterior distribution of the model parameters (updated beliefs about parameters based on both prior information and the observed data). Mathematically, it is expressed as:\n\nP(future data∣observed data)\n\n=∫P(future data∣parameters)⋅(parameters∣observed data) P(parameters\n\nP(future data∣observed data)=∫P(future data∣parameters)⋅P(parameters∣observed data) d parameters\nThis distribution represents the model’s prediction for future data given the observed data.\nPosterior Predictive Power:\nPosterior predictive power is a measure of how well the model’s predictions align with new, unseen data. It helps evaluate the model’s ability to generalize beyond the observed data.\n\n\n\nIt can be assessed using various measures, such as predictive accuracy, calibration, and other model performance metrics.\nImportance in Model Assessment:\n\nModel fitting alone does not guarantee that a model will perform well on new data. Posterior predictive power provides a more comprehensive evaluation by considering both the fit to observed data and the model’s predictive capabilities.\nIt helps identify whether the model is overfitting (fitting noise in the data rather than capturing underlying patterns) or underfitting (oversimplifying the data).\n\nExamples from the Medical Field:\n\nClinical Trials: In medical research, Bayesian models are often used to analyze clinical trial data. Posterior predictive power can be employed to assess how well the model predicts patient outcomes or responses to treatment in new trials.\nDiagnostic Models: For medical diagnostic models, posterior predictive power can be used to evaluate how well the model predicts outcomes for patients not included in the initial dataset. This is crucial for assessing the model’s generalizability in real-world scenarios.\nEpidemiological Models: Bayesian models are used in epidemiology to understand disease spread and predict future outbreaks. Posterior predictive power helps assess the reliability of these models in forecasting the dynamics of infectious diseases.\nHealth Outcome Predictions: In personalized medicine, Bayesian models can be used to predict individual health outcomes based on genetic, clinical, and environmental factors. Posterior predictive power is essential to evaluate the accuracy of these predictions.\n\n\nIn summary, posterior predictive power in Bayesian statistics provides a robust framework for assessing the overall performance of models, especially in fields like medicine where accurate predictions are crucial for patient well-being and decision-making.\n\nMarkov Chain Monte Carlo (MCMC):\nMCMC is a class of algorithms used for sampling from complex probability distributions, which is often encountered in Bayesian statistical inference. The primary goal is to generate samples from the posterior distribution of model parameters given observed data. The central idea is to construct a Markov Chain that, when run for a sufficiently long time, converges to the target posterior distribution.\n\n\nMarkov Chain Values and Their Role:\n\nMarkov Property:\n\nA Markov Chain is a sequence of random variables where the probability distribution of each variable depends only on the preceding variable. This is known as the Markov property.\nIn the context of MCMC, Markov Chains are constructed to explore the parameter space of a Bayesian model. Each value in the chain represents a set of parameter values.\n\nSampling from the Posterior:\n\nMCMC methods generate a sequence of parameter values from the posterior distribution. Each value in the Markov Chain is a sample from the posterior, and the chain is constructed in a way that it explores the high-probability regions of the parameter space.\n\nConvergence and Mixing:\n\nThe quality of MCMC sampling depends on the convergence and mixing properties of the Markov Chain.\nConvergence ensures that the chain has reached a stationary distribution, and further samples provide information about the target posterior.\nMixing refers to how effectively the chain explores the parameter space. A well-mixing chain moves easily between different regions of high posterior probability.\n\nTrace Plots and Diagnostics:\n\nPractitioners often examine trace plots, which show the values of parameters in the Markov Chain over iterations. These plots help assess convergence and identify potential issues.\nDiagnostic tools, such as the Gelman-Rubin statistic, effective sample size, and autocorrelation plots, are used to evaluate the quality of the Markov Chain values.\n\nInference and Uncertainty Quantification:\n\nOnce a well-behaved Markov Chain is obtained, the values in the chain can be used for Bayesian inference.\nPosterior summaries, such as mean, median, and credible intervals, can be computed from the Markov Chain values to quantify uncertainty in parameter estimates.\n\nBayesian Updating:\n\nIn a Bayesian framework, new data can be incorporated by updating the Markov Chain using the posterior distribution based on the observed data. This allows for iterative learning and model refinement.\n\n\n\n\nExample:\nConsider a Bayesian linear regression model with unknown parameters 0 and 1. MCMC methods could generate a Markov Chain of values for  0 and 1based on observed data. The chain’s convergence and mixing properties, as well as the distribution of values in the chain, provide insights into the uncertainty associated with the parameter estimates.\nIn summary, Markov Chain values are the backbone of MCMC algorithms, enabling Bayesian practitioners to sample from complex posterior distributions, conduct inference, and quantify uncertainty in model parameters.\nThe RStan Package\nRStan is an R interface to Stan, which is a probabilistic programming language for Bayesian statistical modeling and high-performance statistical computation. RStan allows users to specify Bayesian models using the Stan language and then perform Bayesian inference using Markov Chain Monte Carlo (MCMC) methods. Keep in mind that software packages may have undergone updates or changes since then, so it’s a good idea to check for the latest information.\nHere’s a general overview of the RStan package and its main functionalities:\n\nInstallation:\n\nTo use RStan, you need to install both RStan and the Stan software. Instructions for installation can be found on the official RStan website.\n\nModel Specification:\n\nRStan allows users to specify Bayesian models using the Stan modeling language. Stan provides a flexible and expressive language for defining hierarchical models, specifying priors, likelihoods, and performing Bayesian inference.\n\nCompilation:\n\nOnce the Stan model is defined in R, it needs to be compiled. RStan facilitates the compilation process, translating the Stan model code into C++ code for efficient computation.\n\nSampling and Inference:\n\nRStan uses MCMC algorithms, particularly the No-U-Turn Sampler (NUTS), for sampling from the posterior distribution of model parameters. Users can control the sampling process, such as the number of iterations and chains, through RStan functions.\n\n\n\n\nPosterior Analysis:\n\nAfter sampling, RStan provides tools for posterior analysis. Users can examine trace plots, summary statistics, and other diagnostics to assess the convergence and performance of the Markov Chain.\n\nVisualization:\n\nRStan includes functions for visualizing results, such as posterior density plots, trace plots, and other diagnostic plots to aid in model assessment.\n\nIntegration with R Ecosystem:\n\nRStan seamlessly integrates with the broader R ecosystem, allowing users to leverage R’s extensive capabilities for data manipulation, visualization, and reporting.\n\nParallelization:\n\nRStan supports parallelization, allowing users to run multiple chains in parallel for faster computation.\n\n\nHere’s a simple example of how you might use RStan to fit a Bayesian linear regression model:\n\n# Install and load the RStan package\n# install.packages(\"rstan\")\n# library(rstan)\n# Define the Stan model\n# stan_code &lt;- \"\n# data {\n#  int&lt;lower=0&gt; N; # Number of observations\n#  vector[N] x; # Predictor variable\n#  vector[N] y; # Response variable\n# }\n# parameters {\n#  real alpha; # Intercept\n#  real beta; # Slope\n#  real&lt;lower=0&gt; sigma; # Standard deviation of the errors\n# }\n# model {\n#  y ~ normal(alpha + beta * x, sigma); # Likelihood\n# }\n#  Additional blocks for priors and other specifications can be added\n# Compile the Stan model\n# stan_model &lt;- stan_model(model_code = stan_code)\n# Create data list\n# data_list &lt;- list(N = length(data$y), x = data$x, y = data$y)\n# Run MCMC sampling\n# stan_fit &lt;- sampling(stan_model, data = data_list, iter = 2000, chains = 4)\n\n# Print summary\n# print(stan_fit)"
  },
  {
    "objectID": "instructors/1-1-probability-1-in-class-examples.html",
    "href": "instructors/1-1-probability-1-in-class-examples.html",
    "title": "Examples for Review of Basic Probability - Part 1",
    "section": "",
    "text": "Events and Sample Spaces\n\nExample 1Example 2Example 3\n\n\n\nSuppose we roll two fair, indistinguishable, dice.\n\nWhat is the sample space?\nDoes order matter for the experiment? Why or why not?\nLet event \\(A\\) be rolling doubles. What are the outcomes that belong to event \\(A\\)?\nLet event \\(B\\) be rolling at least one odd number. What are the outcomes that belong to event \\(B\\)?\n\nTable of possible outcomes when rolling two fair, indistinguishable dice:\n\n\n\n\n\n\n\n\n(1, 1)\n\n\n(1, 2)\n\n\n(1, 3)\n\n\n(1, 4)\n\n\n(1, 5)\n\n\n(1, 6)\n\n\n\n\n\n\n(2, 1)\n\n\n(2, 2)\n\n\n(2, 3)\n\n\n(2, 4)\n\n\n(2, 5)\n\n\n(2, 6)\n\n\n\n\n(3, 1)\n\n\n(3, 2)\n\n\n(3, 3)\n\n\n(3, 4)\n\n\n(3, 5)\n\n\n(3, 6)\n\n\n\n\n(4, 1)\n\n\n(4, 2)\n\n\n(4, 3)\n\n\n(4, 4)\n\n\n(4, 5)\n\n\n(4, 6)\n\n\n\n\n(5, 1)\n\n\n(5, 2)\n\n\n(5, 3)\n\n\n(5, 4)\n\n\n(5, 5)\n\n\n(5, 6)\n\n\n\n\n(6, 1)\n\n\n(6, 2)\n\n\n(6, 3)\n\n\n(6, 4)\n\n\n(6, 5)\n\n\n(6, 6)\n\n\n\n\n\n\n\n\n\nConsider the word TENNESSEE. Suppose we were to randomly select a letter.\n\nWhat is the sample space?\nWhat is \\(P(T)\\)?\nWhat is \\(P(E)\\)?\nWhat is \\(P(N)\\)?\nWhat is \\(P(S)\\)?\n\n\n\n\n\nSet up: split class into various groups.\nSuppose we roll two indistinguishable fair dice. We are interested in the sum of the numbers on the two dice.\n\nWhat does the sample space become?\nDoes order matter for the experiment? Why or why not?\nLet event \\(A\\) be rolling an even sum. What are the outcomes that belong to event \\(A\\)?\n\nWhat is \\(P(A)\\)?\n\nLet event \\(B\\) be rolling a sum that is a prime number. What are the outcomes that belong to event \\(B\\)?\n\nWhat is \\(P(B)\\)?\n\nAssign each group an event and have them find the probabilities.\n\n\\(C\\): roll a sum that is an odd sum (bonus: complement rule early :))\n\\(D\\): roll a sum that is (strictly) less than 5.\n\\(E\\): roll an even sum that is 9 or greater.\netc.\n\nHave one member of each group present the set of outcomes belonging to the event.\nHave another member present how to find the probability.\n\nTable of possible outcomes:\n\n\n\n\n\n\n\n\n(1, 1) = 2\n\n\n(1, 2) = 3\n\n\n(1, 3) = 4\n\n\n(1, 4) = 5\n\n\n(1, 5) = 6\n\n\n(1, 6) = 7\n\n\n\n\n\n\n(2, 1) = 3\n\n\n(2, 2) = 4\n\n\n(2, 3) = 5\n\n\n(2, 4) = 6\n\n\n(2, 5) = 7\n\n\n(2, 6) = 8\n\n\n\n\n(3, 1) = 4\n\n\n(3, 2) = 5\n\n\n(3, 3) = 6\n\n\n(3, 4) = 7\n\n\n(3, 5) = 8\n\n\n(3, 6) = 9\n\n\n\n\n(4, 1) = 5\n\n\n(4, 2) = 6\n\n\n(4, 3) = 7\n\n\n(4, 4) = 8\n\n\n(4, 5) = 9\n\n\n(4, 6) = 10\n\n\n\n\n(5, 1) = 6\n\n\n(5, 2) = 7\n\n\n(5, 3) = 8\n\n\n(5, 4) = 9\n\n\n(5, 5) = 10\n\n\n(5, 6) = 11\n\n\n\n\n(6, 1) = 7\n\n\n(6, 2) = 8\n\n\n(6, 3) = 9\n\n\n(6, 4) = 10\n\n\n(6, 5) = 11\n\n\n(6, 6) = 12\n\n\n\n\n\n\n\n\n\n\n\nAddition Rule for Mutually Exclusive Events\n\nExample 1Example 2Example 3\n\n\n\nSuppose a single card is drawn from a standard 52-card deck.\n\nWhat is the sample space?\n\nSuppose event \\(A\\) is drawing a face card (J, Q, K).\n\nWhat is \\(P(A)\\)?\n\nSuppose event \\(B\\) is drawing a black, even card.\n\nWhat is \\(P(B)\\)?\n\nAre \\(A\\) and \\(B\\) mutually exclusive events? Why or why not?\n\nWhat is \\(P(A \\cup B)\\)?\n\n\nTable of card deck:\n\n\n\n\n\n\n\nClubs:\n\n\nA\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\nJ\n\n\nQ\n\n\nK\n\n\n\n\n\n\nSpades:\n\n\nA\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\nJ\n\n\nQ\n\n\nK\n\n\n\n\nDiamonds:\n\n\nA\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\nJ\n\n\nQ\n\n\nK\n\n\n\n\nHearts:\n\n\nA\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\nJ\n\n\nQ\n\n\nK\n\n\n\n\n\n\n\n\n\nSuppose we toss three coins.\n\nWhat is the sample space?\n\nLet event \\(A\\) be flipping at least two tails.\n\nWhat is \\(P(A)\\)?\n\nLet event \\(B\\) be flipping no tails.\n\nWhat is \\(P(B)\\)?\n\nLet event \\(C\\) be flipping no heads.\n\nWhat is \\(P(C)\\)?\n\nAre the following events mutually exclusive? Why or why not?\n\n\\(A\\) and \\(B\\)\n\\(A\\) and \\(C\\)\n\\(B\\) and \\(C\\)\n\n\nSample Space:\n\n\n\n\n\n\n\nHHH\n\n\n\n\n\n\n\n\n\n\nHHT\n\n\nHTH\n\n\nTHH\n\n\n\n\nHTT\n\n\nTHT\n\n\nTTH\n\n\n\n\nTTT\n\n\n\n\n\n\n\n\n\n\n\n\n\nSet up: split class into various groups.\nSuppose we are rolling two dice: one red, one blue.\nAssign each group an event and have them find the probabilities:\n\n\\(A\\): Rolling a red 2.\n\\(B\\): Rolling a blue 5.\n\\(C\\): Rolling a red odd.\n\\(D\\): Rolling a sum that is odd.\n\\(E\\): Rolling a sum that is even.\netc.\n\nHave groups determine which other groups they are mutually exclusive with.\nHave groups find other groups they are mutually exclusive with and find \\(P(E_1 \\cup E_2)\\).\n\n\n\n\n\n\nExamples for the General Addition Rule\n\nExample 1Example 2Example 3Example 4\n\n\n\nThe probability of a teenager owning a Playstation is 0.31, of owning a Switch is 0.56 and of owning both is 0.17.\n\nWhat are the events that are defined by the problem?\n\nIf a teenager is chosen at random, what is the probability that the teenager owns a Playstation or Switch?\n\nWhat is \\(P(\\text{Playstation})\\)?\nWhat is \\(P(\\text{Switch})\\)?\nWhat is \\(P(\\text{Playstation} \\cap \\text{Switch})\\)?\nWhat is \\(P(\\text{Playstation} \\cup \\text{Switch})\\)?\n\nSuggestion: Venn Diagram\n\n\n\n\nThere are 100 students taking either STA4173 (Biostatistics) or STA4231 (Statistics for Data Science I). 80 students are taking Biostatistics and 30 students are taking Statistics for Data Science I.\n\nWhat is \\(P(\\text{Biostatistics})\\)?\nWhat is \\(P(\\text{Statistics for Data Science I})\\)?\nWhat is \\(P(\\text{Biostatistics} \\cap \\text{Statistics for Data Science I})\\)?\nWhat is \\(P(\\text{Biostatistics} \\cup \\text{Statistics for Data Science I})\\)?\n\nSuggestion: Venn Diagram\n\n\n\n\nSuppose a single card is drawn from a standard 52-card deck.\n\nWhat is the sample space?\n\nSuppose event \\(A\\) is drawing a face card (J, Q, K).\n\nWhat is \\(P(A)\\)?\n\nSuppose event \\(B\\) is drawing a red card.\n\nWhat is \\(P(B)\\)?\n\nAre \\(A\\) and \\(B\\) mutually exclusive events? Why or why not?\nWhat is \\(P(A \\cup B)\\)?\n\nTable of card deck:\n\n\n\n\n\n\n\nClubs:\n\n\nA\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\nJ\n\n\nQ\n\n\nK\n\n\n\n\n\n\nSpades:\n\n\nA\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\nJ\n\n\nQ\n\n\nK\n\n\n\n\nDiamonds:\n\n\nA\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\nJ\n\n\nQ\n\n\nK\n\n\n\n\nHearts:\n\n\nA\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\nJ\n\n\nQ\n\n\nK\n\n\n\n\n\n\n\n\n\nSet up: split class into various groups.\nSuppose two cards are drawn without replacement from a standard 52-card deck.\nAssign each group an event and have them find the corresponding probabilities.\n\n\\(A\\): drawing two even cards\n\\(B\\): drawing two face cards\n\\(C\\): drawing a red 2 and black 3\netc.\n\nHave one student from each group present their probabilities.\nPair groups together and ask them to find \\(P(E_1 \\cup E_2)\\).\nHave one student from each paired group present their solution.\n\nTable of card deck:\n\n\n\n\n\n\n\nClubs:\n\n\nA\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\nJ\n\n\nQ\n\n\nK\n\n\n\n\n\n\nSpades:\n\n\nA\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\nJ\n\n\nQ\n\n\nK\n\n\n\n\nDiamonds:\n\n\nA\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\nJ\n\n\nQ\n\n\nK\n\n\n\n\nHearts:\n\n\nA\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\nJ\n\n\nQ\n\n\nK"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayesian Thinking",
    "section": "",
    "text": "Topic\nLecture Notes\nIn Class Notes for Students\n\n\n\n\nProbability\npart 1 lesson plan, part 2 lesson plan\npart 1 examples, part 2 examples\n\n\nProbability Distributions\nlesson plan\nexamples\n\n\nBayes Theorem\nlesson plan\nexamples\n\n\nDistributions: Priors\nlesson plan\nexamples\n\n\nDistributions: Data\nlesson plan\nexamples\n\n\nBeta-Binomial\nlesson plan\nexamples\n\n\nNormal-Normal\nlesson plan\nexamples\n\n\nMCMC\nlesson plan\nexamples\n\n\nGibbs Sampler\nlesson plan\nexamples\n\n\nPosterior Predictions\nlesson plan\nexamples\n\n\n\n\nLecture Notes are intended for instructors and contain information about the topic.\nIn Class Notes for Students are notes skeletons for students to actively engage with the programming aspect during lecture."
  },
  {
    "objectID": "examples.html",
    "href": "examples.html",
    "title": "Bayesian Thinking",
    "section": "",
    "text": "Topic\nNotes\nExamples\n\n\n\n\nIntroduction to Technology\n\n\n\n\nProbability\n\n\n\n\nProbability Distributions\n\n\n\n\nBayes Theorem\n\n\n\n\nDistributions: Priors\n\n\n\n\nDistributions: Data\n\n\n\n\nBeta-Binomial\n\n\n\n\nNormal-Normal\n\n\n\n\nMCMC & Gibbs Sampler\n\n\n\n\nPredictions"
  },
  {
    "objectID": "instructors/4-0-Bayes-theorem-lesson-plan.html",
    "href": "instructors/4-0-Bayes-theorem-lesson-plan.html",
    "title": "Introduction to Bayes’ Theorem",
    "section": "",
    "text": "Objective:\n\nBy the end of this lesson, students will be able to understand the concept of Bayes’ Theorem, its historical context, and its applications in probability and statistics.\n\nDuration:\n\nAbout 60 minutes\n\nMaterials:\n\nWhiteboard and markers\nHandouts with exercises and problems related to Bayes’ Theorem\nProjector and screen\n\nIntroduction:\n\nBegin the lesson with a question to engage students: “Have you ever wondered how we make decisions based on uncertain information?”\nDiscuss a few examples, like medical diagnoses, weather forecasts, or financial decisions.\n\nIntroduce the lesson’s topic:\n\nBayes’ Theorem, a fundamental concept in probability theory that helps us update our beliefs based on new evidence.\n\nHistorical Context:\n\nBayes’ Theorem is named after Reverend Thomas Bayes, an 18th-century English statistician, philosopher, and theologian.\nThomas Bayes never published his work on the theorem during his lifetime. It was discovered in his notes after his death and published posthumously.\nStory of Thomas Bayes:\n\nThomas Bayes was born in 1701 and is believed to have been largely self-educated in mathematics and logic.\nHe became a Presbyterian minister, and his interests in theology and mathematics intersected in intriguing ways.\nWhile not widely known during his lifetime, his work laid the foundation for modern probability theory.\n\n\nMain Content: Bayes’ Theorem\nBasic Probability Concepts:\n\nDefine key terms: prior probability, posterior probability, likelihood, evidence, and conditional probability.\nProvide examples to illustrate these concepts, such as rolling dice or drawing cards from a deck.\n\nUnderstanding Bayes’ Theorem:\n\nPresent the formula for Bayes’ Theorem: \\[ \\text{P}( A | B ) = \\frac{\\text{P}( B | A ) \\text{P}(A)}{\\text{P}(B)} \\]\nDiscuss how Bayes’ Theorem allows us to update our beliefs about an event \\(A\\) given new evidence \\(B\\).\n\nApplications of Bayes’ Theorem:\n\nMedical diagnosis (e.g., disease testing)\nCriminal investigations (e.g., DNA evidence)\nSpam email filtering\nWeather forecasting\n\nCalculation and Practice:\n\nGuide students through solving example problems using Bayes’ Theorem.\nDistribute handouts with exercises and problems for individual or group work.\n\nDiscussion and Wrap-Up:\n\nFacilitate a class discussion to review the example problems and reinforce key concepts.\nAsk students about their thoughts on how Bayes’ Theorem can be applied in their daily lives or in other fields of study.\n\nHomework:\n\nAssign additional problems from the textbook or handouts to reinforce the understanding of Bayes’ Theorem.\n\nFormative Assessment:\n\nEvaluate students based on their participation in discussions, their ability to solve example problems, and their performance on the assigned homework.\n\nConclusion:\n\nLet students summarize the importance of Bayes’ Theorem in making informed decisions under uncertainty.\nEncourage students to explore further applications and study advanced topics in probability and statistics."
  },
  {
    "objectID": "instructors/5-1-priors-in-class-examples.html",
    "href": "instructors/5-1-priors-in-class-examples.html",
    "title": "Examples for Prior Distributions",
    "section": "",
    "text": "library(bayesrules)\nlibrary(tidyverse)"
  },
  {
    "objectID": "instructors/5-1-priors-in-class-examples.html#language-notes-about-nomenclature",
    "href": "instructors/5-1-priors-in-class-examples.html#language-notes-about-nomenclature",
    "title": "Examples for Prior Distributions",
    "section": "Language / notes about nomenclature",
    "text": "Language / notes about nomenclature"
  },
  {
    "objectID": "instructors/5-1-priors-in-class-examples.html#beta-prior",
    "href": "instructors/5-1-priors-in-class-examples.html#beta-prior",
    "title": "Examples for Prior Distributions",
    "section": "Beta Prior",
    "text": "Beta Prior\n\nReview the beta distribution\nSuppose we are looking at binary outcomes; we want to put a prior on \\(\\pi = P[Y=1]\\), meaning \\(\\pi \\in [0, 1]\\).\nThe Beta model (often used to describe the variability in \\(\\pi\\)) has shape parameters \\(\\alpha &gt; 0\\) and \\(\\beta &gt; 0\\), and these are the shape hyperparameters.\n\\[\\pi \\sim \\text{Beta}\\left(\\alpha, \\beta \\right),\\]\nThe Beta model’s pdf is\n\\[f\\left( \\pi \\right) = \\frac{\\Gamma \\left( \\alpha + \\beta \\right)}{\\Gamma \\left( \\alpha \\right) \\Gamma \\left( \\beta \\right)} \\pi^{\\alpha-1} (1-\\pi)^{\\beta-1},\\]\n\nNote the following:\n\n\\(\\Gamma\\left( z \\right) = \\int_{0}^{\\infty} x^{z-1} e^{-y} dx\\)\n\\(\\Gamma\\left( z + 1 \\right) = z \\Gamma\\left( z \\right)\\)\nif \\(z\\in \\mathbb{Z}^+\\), then \\(\\Gamma\\left( z \\right) = (z-1)!\\)\n\n\n\nExample 1Example 2Example 3\n\n\nLet’s play with plotting the Beta distribution.\n\nplot_beta(5, 5)\n\n\n\nplot_beta(15, 15)\n\n\n\nplot_beta(30, 30)\n\n\n\nplot_beta(2, 5)\n\n\n\nplot_beta(5, 2)\n\n\n\nplot_beta(2,1)\n\n\n\n\n\n\nJillian is a soccer player who, throughout her career in competitive soccer, has probability 0.5 of scoring when she attempts. However, her coach suspects that she’s getting better. Her coach begins keeping tabs and is going to ask us to analyze the data. While we wait for them to complete data collection, we can go ahead and decide on the prior distribution.\nThinking about our original exploration of the Beta distribution,\n\nplot_beta(5, 5)\n\n\n\nplot_beta(15, 15)\n\n\n\nplot_beta(30, 30)\n\n\n\n\nThe above distributions are centered at 0.5, we just need to decide on which is best for our analysis. What should we choose?\n\n\nSuppose we are estimating the population proportion of children with vanishing white matter disease, a rare disease. What should our prior be?"
  },
  {
    "objectID": "instructors/5-1-priors-in-class-examples.html#normal-prior",
    "href": "instructors/5-1-priors-in-class-examples.html#normal-prior",
    "title": "Examples for Prior Distributions",
    "section": "Normal Prior",
    "text": "Normal Prior\n\nReview the normal distribution\nSuppose we are now examining a continuous outcome. Let \\(Y\\) be a continuous random variable that can take any value in \\(\\mathbb{R}\\); i.e., \\(Y \\in \\left(-\\infty, \\infty\\right)\\).\nLet us assume that the variability in \\(Y\\) can be represented by the normal distribution with mean parameter \\(\\mu \\in \\mathbb{R}\\) and standard deviation parameter \\(\\sigma \\in \\mathbb{R}^+\\).\n\\[Y \\sim N\\left(\\mu, \\sigma^2\\right)\\]\nThe normal model’s pdf is\n\\[f(y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left\\{-\\frac{\\left(y - \\mu\\right)^2}{2\\sigma^2} \\right\\}\\]\n\nNote: \\(\\sigma\\) provides a sense of scale for \\(Y\\); approximately 95% of \\(Y\\) values will be within 2 standard deviations.\n\ni.e., \\(\\mu \\pm 2 \\sigma\\)\n\n\n\nExample 1Example 2Example 3\n\n\nWhat happens as variability increases?\n\nplot_normal(0, 1) + ylim(0,0.5)\n\n\n\nplot_normal(0, 10) + ylim(0,0.5)\n\n\n\nplot_normal(0, 100) + ylim(0,0.5)\n\n\n\n\nWhat happens as variability decreases?\n\nplot_normal(0, 1) + ylim(0, 1)\n\n\n\nplot_normal(0, 0.75) + ylim(0, 1)\n\n\n\nplot_normal(0, 0.5) + ylim(0, 1)\n\n\n\n\n\n\nA new Introduction to Biostatistics instructor believes that exam grades should follow a normal distribution. Construct the following priors; remember to keep the y-axis on the same scale for all graphs.\nA prior with \\(\\mu=75\\) and high variability:\nA prior with \\(\\mu=75\\) and medium variability:\nA prior with \\(\\mu=75\\) and low variability:\n\n\nLet \\(\\mu\\) be the average 5 p.m. temperature in Pensacola. Dr. Seals believes that Pensacola is hot year round, so she believes that the average temperature is probably around 85 degrees Fahrenheit. However, you are skeptical that it is hot all year, so let’s come up with a prior for this."
  },
  {
    "objectID": "instructors/2-0-probability-2-lesson-plan.html",
    "href": "instructors/2-0-probability-2-lesson-plan.html",
    "title": "Review of Basic Probability - Part 2",
    "section": "",
    "text": "Objective:\n\nBy the end of this lesson, students will have reviewed the following topics:\n\nIndependent/dependent events\nMultiplication Rule for independent events\nConditional probability\nGeneral Multiplication Rule for dependent events\n\n\nDuration:\n\n75 minutes\n\nMaterials:\n\nHandouts with exercises and problems related to basic probability\nComputer, projector, and screen\n\nIntroduction:\n\nExamples that can be used to jump start topic:\n\nbirthday problem\nMonty Hall problem\ncasino games\n\n\nIntroduce the lesson’s topic:\n\nToday we will continue reviewing basic probability rules.\n\nHistorical Context:\n\n1560s: Cardano wrote Liber de ludo aleae, the first known systematic treatment of probability, and as the result of a gambling addiction.\n1654: Fermat and Pascal worked on the foundation of probability theory through correspondence.\n1812 and 1814: Laplace published Théorie analytique des probabilités and Essai philosophique sur les probabilités, outlining many basic and fundamental results in statistics.\n\nMain Content:\n\nConditional probability: if \\(A\\) and \\(B\\) are any two events,\n\n\\[P(A|B) = \\frac{P(A \\cap B)}{P(A)}\\]\n\nGeneral Multiplication Rule for dependent events: the probability that two events \\(A\\) and \\(B\\) both occur is\n\n\\[P(A \\cap B) = P(A) \\times P(A|B)\\]\n\nIndependent/dependent events: two events, \\(A\\) and \\(B\\) are independent if\n\n\\[P(A|B) = P(A) \\text{ or } P(B|A) = P(B)\\]\n\nMultiplication Rule for independent events: when two events are independent,\n\n\\[P(A \\cap B) = P(A) \\times P(B)\\]\nDiscussion and Wrap-Up:\n\nFacilitate a class discussion to review the example problems, reinforce key concepts, and answer any questions the students have.\n\nHomework:\n\nAssign additional problems to practice the basic probability rules.\n\nFormative Assessment:\n\nEvaluate students based on their participation in discussions, their ability to solve example problems, and their performance on the assigned homework.\n\nConclusion:\n\nEmphasize these are building blocks for the next lesson and long term understanding probability."
  },
  {
    "objectID": "instructors/5-0-priors-lesson-plan.html",
    "href": "instructors/5-0-priors-lesson-plan.html",
    "title": "Prior Distributions",
    "section": "",
    "text": "Objective:\n\nBy the end of this lesson, students will have reviewed the following topics:\n\nDefinition of prior distribution\nDefinition of hyperparameter\nIdentify appropriate distribution to represent belief\n\n\nDuration:\n\n75 minutes\n\nMaterials:\n\nHandouts with exercises and problems related to basic probability\nComputer, projector, and screen\n\nIntroduction:\n\nWhat should the probability distribution be for the maximum temperature 1 year from today?\nWhat should the probability distribution be for the final grade in this course?\n\nIntroduce the lesson’s topic:\n\nToday we will apply our knowledge of distributions to represent our prior beliefs.\n\nMain Content:\nVocabulary\n\nPrior distribution: probability model for our prior understanding of \\(\\text{P}[\\text{event}]\\).\n\nHyperparameter: parameter in the specified prior distribution.\n\nData distribution: probability model for the outcome, \\(y\\).\n\nParameter: parameter in the specified data distribution.\n\nPosterior distribution: probability model that summarizes the plausibility of the outcome, \\(y\\), given the prior information.\n\nBeta distribution\nSuppose we are looking at binary outcomes; we want to put a prior on \\(\\pi = P[Y=1]\\), meaning \\(\\pi \\in [0, 1]\\).\nThe Beta model (often used to describe the variability in \\(\\pi\\)) has shape parameters \\(\\alpha &gt; 0\\) and \\(\\beta &gt; 0\\), and these are the shape hyperparameters.\n\\[\\pi \\sim \\text{Beta}\\left(\\alpha, \\beta \\right),\\]\nThe Beta model’s pdf is\n\\[f\\left( \\pi \\right) = \\frac{\\Gamma \\left( \\alpha + \\beta \\right)}{\\Gamma \\left( \\alpha \\right) \\Gamma \\left( \\beta \\right)} \\pi^{\\alpha-1} (1-\\pi)^{\\beta-1},\\]\n\nNote the following:\n\n\\(\\Gamma\\left( z \\right) = \\int_{0}^{\\infty} x^{z-1} e^{-y} dx\\)\n\\(\\Gamma\\left( z + 1 \\right) = z \\Gamma\\left( z \\right)\\)\nif \\(z\\in \\mathbb{Z}^+\\), then \\(\\Gamma\\left( z \\right) = (z-1)!\\)\n\n\nNormal distribution\nSuppose we are now examining a continuous outcome. Let \\(Y\\) be a continuous random variable that can take any value in \\(\\mathbb{R}\\); i.e., \\(Y \\in \\left(-\\infty, \\infty\\right)\\).\nLet us assume that the variability in \\(Y\\) can be represented by the normal distribution with mean parameter \\(\\mu \\in \\mathbb{R}\\) and standard deviation parameter \\(\\sigma \\in \\mathbb{R}^+\\).\n\\[Y \\sim N\\left(\\mu, \\sigma^2\\right)\\]\nThe normal model’s pdf is\n\\[f(y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left\\{-\\frac{\\left(y - \\mu\\right)^2}{2\\sigma^2} \\right\\}\\]\n\nNote: \\(\\sigma\\) provides a sense of scale for \\(Y\\); approximately 95% of \\(Y\\) values will be within 2 standard deviations.\n\ni.e., \\(\\mu \\pm 2 \\sigma\\)\n\n\nCalculation and Practice:\n\nExamples for the Beta distribution:\n\nExample 1: plotting with different parameters\nExample 2: students evaluate and choose between three distributions\nExample 3: students derive appropriate prior\n\nExamples for the normal distribution:\n\nExample 1: what happens as variability changes?\nExample 2: students construct appropriate normals with given mean\nExample 3: students derive appropriate prior\n\n\nDiscussion and Wrap-Up:\n\nFacilitate a class discussion to review the example problems, reinforce key concepts, and answer any questions the students have.\n\nHomework:\n\nAssign additional problems to practice the basic probability rules.\n\nFormative Assessment:\n\nEvaluate students based on their participation in discussions, their ability to solve example problems, and their performance on the assigned homework.\n\nConclusion:\n\nEmphasize that a prior will not make or break an analysis.\nOur goal is to analyze in the best way possible."
  },
  {
    "objectID": "instructors/3-0-probability-distribution-lesson-plan.html",
    "href": "instructors/3-0-probability-distribution-lesson-plan.html",
    "title": "Lesson Plan: Probability Distributions",
    "section": "",
    "text": "By the end of this lesson, students will be able to:\n\nIdentify and define discrete and continuous random variables given a real-world application.\nDefine and identify properties of discrete and continuous probability distributions.\nCreate a discrete probability distribution and graph it.\nCalculate the area of continuous distributions utilizing geometric area formulas with a given graph. (Calculus approach could be done depending on the math prerequisite of the course.)\nCompute and label incomplete axes with values utilizing their understanding that the area of all probability distributions is equal to one."
  },
  {
    "objectID": "instructors/3-0-probability-distribution-lesson-plan.html#objective",
    "href": "instructors/3-0-probability-distribution-lesson-plan.html#objective",
    "title": "Lesson Plan: Probability Distributions",
    "section": "",
    "text": "By the end of this lesson, students will be able to:\n\nIdentify and define discrete and continuous random variables given a real-world application.\nDefine and identify properties of discrete and continuous probability distributions.\nCreate a discrete probability distribution and graph it.\nCalculate the area of continuous distributions utilizing geometric area formulas with a given graph. (Calculus approach could be done depending on the math prerequisite of the course.)\nCompute and label incomplete axes with values utilizing their understanding that the area of all probability distributions is equal to one."
  },
  {
    "objectID": "instructors/3-0-probability-distribution-lesson-plan.html#getting-started",
    "href": "instructors/3-0-probability-distribution-lesson-plan.html#getting-started",
    "title": "Lesson Plan: Probability Distributions",
    "section": "Getting Started:",
    "text": "Getting Started:\nDuration: - 75 minutes\nMaterials:\n\nHandouts with exercises and problems\nComputer, projector, and screen\nwhiteboard and markers\n\nIntroduction:\nBegin class with the Notice and Wonder activity. State the objectives of the lesson. Then, give a brief historical connection."
  },
  {
    "objectID": "instructors/3-0-probability-distribution-lesson-plan.html#notice-and-wonder",
    "href": "instructors/3-0-probability-distribution-lesson-plan.html#notice-and-wonder",
    "title": "Lesson Plan: Probability Distributions",
    "section": "Notice and Wonder:",
    "text": "Notice and Wonder:\nHave students look at two probability distributions and ask them what they notice about the graphs and what they wonder. If students have never done this type of activity, you will need to provide a longer wait time for the students to respond. Be accepting of all responses.\n\n\n\n\n\n\n\n\n\n\nSample Student Response:\nBoth graphs have the year on the x-axis. Both graphs have the same scale on the y-axis. The right graph is a darker color. Both graphs have the same number of bars. The graph on the left has the bars touching. The left graph is a histogram, and the right is a bar graph. I wonder if the right graph is a bar graph because it looks like both axes are quantitative. Maybe someone will think that the left graph can graph continuous data and the right graph is discrete data. I wonder what this graph is counting in each of the years. There is no title, and the y-axis is not labeled."
  },
  {
    "objectID": "instructors/3-0-probability-distribution-lesson-plan.html#historical-context",
    "href": "instructors/3-0-probability-distribution-lesson-plan.html#historical-context",
    "title": "Lesson Plan: Probability Distributions",
    "section": "Historical Context:",
    "text": "Historical Context:\n\n\n\n\n\nGirolamo Cardano\n\n\n\nGirolamo Cardano was a 16th-century Italian professor of mathematics and medicine, but he was also a gambler. He could have been the father of probability if his work had been published when he first recorded his ideas. Instead, his work was published centuries later. He was perhaps the first to think about assigning a number from 0 to 1 to the probability of an outcome."
  },
  {
    "objectID": "instructors/3-0-probability-distribution-lesson-plan.html#jakob-bernoulli",
    "href": "instructors/3-0-probability-distribution-lesson-plan.html#jakob-bernoulli",
    "title": "Lesson Plan: Probability Distributions",
    "section": "Jakob Bernoulli",
    "text": "Jakob Bernoulli\n\n\n\n\n\nJakob Bernoulli\n\n\n\nThis was lucky for Jakob Bernoulli, a Swiss mathematician, who then received the credit for introducing the idea of representing complete certainty as one and probability as a number between zero and one with his publication of Ars Conjectandi (posthumous, 1713)."
  },
  {
    "objectID": "instructors/3-0-probability-distribution-lesson-plan.html#main-content",
    "href": "instructors/3-0-probability-distribution-lesson-plan.html#main-content",
    "title": "Lesson Plan: Probability Distributions",
    "section": "Main Content:",
    "text": "Main Content:\nA probability distribution is a statistical function describing the probability (likelihood) of obtaining all possible values a random variable can take. Typically, we use graphs or tables as visual representations of probability distributions.\nThere are two types of probability distributions based on the two types of random variables: discrete and continuous. \n\n\n\nFigure 5 Overview of Probability Distributions"
  },
  {
    "objectID": "instructors/3-0-probability-distribution-lesson-plan.html#discrete-random-variable",
    "href": "instructors/3-0-probability-distribution-lesson-plan.html#discrete-random-variable",
    "title": "Lesson Plan: Probability Distributions",
    "section": "Discrete Random Variable",
    "text": "Discrete Random Variable\nA discrete random variable,X, has possible values that can be given in an ordered list. The probability mass function is the probability distribution pi of X lists the values and their given probabilities.\nThe probabilities pi must satisfy two requirements:\n1.     Every probability pi is a number on the interval [0,1].\n2.     p1+p2+p3 … = 1\n\n\n\n\n\n\n\n   Value of X   \n\n\n   x1   \n\n\n   x2   \n\n\n   x3   \n\n\n   …   \n\n\n\n\n   Probability   \n\n\n   p1   \n\n\n   p2   \n\n\n   p3   \n\n\n   …"
  },
  {
    "objectID": "instructors/3-0-probability-distribution-lesson-plan.html#discrete-random-variable-example-1",
    "href": "instructors/3-0-probability-distribution-lesson-plan.html#discrete-random-variable-example-1",
    "title": "Lesson Plan: Probability Distributions",
    "section": "Discrete Random Variable: Example 1:",
    "text": "Discrete Random Variable: Example 1:\n\n\nA discrete probability distribution for countries with significant volcanic eruptions can be represented with a bar graph and/or a table.\n\n\n\n\nFigure 6 Volcanic Eruptions Worldwide 2000 - 2023"
  },
  {
    "objectID": "instructors/3-0-probability-distribution-lesson-plan.html#continuous-random-variable",
    "href": "instructors/3-0-probability-distribution-lesson-plan.html#continuous-random-variable",
    "title": "Lesson Plan: Probability Distributions",
    "section": "Continuous Random Variable",
    "text": "Continuous Random Variable\nA continuous random variable X takes all values in an interval of numbers. A density curve describes the probability distribution of X.  The probability of any event is the area under the density curve and above the values of X that make up the event. \nThe total area of a continuous probability distribution is equal to 1. Because the probability is equal to the area under the curve, all continuous probability distributions assign a probability of 0 to every individual outcome.  Only intervals of values have a positive probability."
  },
  {
    "objectID": "instructors/3-0-probability-distribution-lesson-plan.html#continuous-random-variable-normal-distribution",
    "href": "instructors/3-0-probability-distribution-lesson-plan.html#continuous-random-variable-normal-distribution",
    "title": "Lesson Plan: Probability Distributions",
    "section": "Continuous Random Variable: Normal Distribution",
    "text": "Continuous Random Variable: Normal Distribution\n\n\n\n\n\nGauss (Wittmann & Oreshina,2009)\n\n\n\nA continuous distribution you may be familiar with from the introductory statistics course you had before this course is the Normal Distribution. The probability density function is:\n\\[\nf(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^{2}\\right)\n\\]\nWhere the parameters \\({\\sigma}\\), is the standard deviation, and µ is the mean. This function is used when graphing a normal distribution.\nCarl Friedrich Gauss\nSometimes, the Normal distribution is called a Gaussian distribution because Carl Friedrich Gauss invented it."
  },
  {
    "objectID": "instructors/3-0-probability-distribution-lesson-plan.html#calculation-and-practice",
    "href": "instructors/3-0-probability-distribution-lesson-plan.html#calculation-and-practice",
    "title": "Lesson Plan: Probability Distributions",
    "section": "Calculation and Practice:",
    "text": "Calculation and Practice:\nNormal Distribution Problem\n\n\n\n\n\nFigure 8\n\n\n\nQuestion 1: Based on the graph in Figure 8, what is the probability of having a value between 1 and 2 standard deviations above the mean by looking at the area under the curve for this region. \n Solution: In this case, it is approximately .1359 or 13.59% chance. \nQuestion 2: What is the probability of having a value between two standard deviations below the mean and one standard deviation above the mean?"
  },
  {
    "objectID": "instructors/3-0-probability-distribution-lesson-plan.html#calculation-and-practice-question-2-solution",
    "href": "instructors/3-0-probability-distribution-lesson-plan.html#calculation-and-practice-question-2-solution",
    "title": "Lesson Plan: Probability Distributions",
    "section": "Calculation and Practice: Question 2 Solution",
    "text": "Calculation and Practice: Question 2 Solution\n\n\n\n\n\n\nFigure 9\n\n\n\n\n\nSolution: .1359+ .3413 +.3413 =.8185"
  },
  {
    "objectID": "instructors/3-0-probability-distribution-lesson-plan.html#continuous-random-variable-uniform-distribution",
    "href": "instructors/3-0-probability-distribution-lesson-plan.html#continuous-random-variable-uniform-distribution",
    "title": "Lesson Plan: Probability Distributions",
    "section": "Continuous Random Variable: Uniform Distribution",
    "text": "Continuous Random Variable: Uniform Distribution\n\n\n\n\n\n\nFigure 10\n\n\n\n\nAnother common continuous distribution is the Uniform Distribution (a,b).\nThe area between a and b is = 1."
  },
  {
    "objectID": "instructors/3-0-probability-distribution-lesson-plan.html#calculation-and-practice-question-3",
    "href": "instructors/3-0-probability-distribution-lesson-plan.html#calculation-and-practice-question-3",
    "title": "Lesson Plan: Probability Distributions",
    "section": "Calculation and Practice: Question 3",
    "text": "Calculation and Practice: Question 3\nQuestion 3 What is the area of a Uniform (0,1) distribution pictured below?\n\n\n\nFigure 11: Uniform(0,1)"
  },
  {
    "objectID": "instructors/3-0-probability-distribution-lesson-plan.html#calculation-and-practice-question-3-solution",
    "href": "instructors/3-0-probability-distribution-lesson-plan.html#calculation-and-practice-question-3-solution",
    "title": "Lesson Plan: Probability Distributions",
    "section": "Calculation and Practice: Question 3 Solution",
    "text": "Calculation and Practice: Question 3 Solution\n\n\n\n\n\nSolution: Area of Rectangle = base × height = 1. (Remember the area is defined to be 1. Even though you do not know the height of the rectangle, you still know its area. In a few problems we will use this idea to solve for the missing dimension.)"
  },
  {
    "objectID": "instructors/3-0-probability-distribution-lesson-plan.html#calculation-and-practice-question-4-solutions",
    "href": "instructors/3-0-probability-distribution-lesson-plan.html#calculation-and-practice-question-4-solutions",
    "title": "Lesson Plan: Probability Distributions",
    "section": "Calculation and Practice: Question 4 & Solutions",
    "text": "Calculation and Practice: Question 4 & Solutions\n\n\nQuestion 4 Using a Uniform [0,1] distribution, what would the probability be that the random number was:\n\n\n\n\nFigure 12: Uniform(0,1)\n\n\n\n\nPart a) between .2 and .6?\nSolution Area of Rectangle= base × height = (.6-.2)(1-0) =.4 × 1 =.4 is the probability\nPart b) greater than .75?\n Solution Area of Rectangle= base × height =(1-.75)( 1-0) =.25 × 1 =.25 is the probability. \nPart c) less than .3 or greater than .9?\nSolution Area of Rectangle= base × height But here we have 2 regions so that would mean we need to sum the areas of both regions. Sum or the Area of Two Rectangles = (.3-0)(1-0) + (1-.9)(1-0) = .3 + .1 =.4 is the probability"
  },
  {
    "objectID": "instructors/3-0-probability-distribution-lesson-plan.html#normalizing-constant",
    "href": "instructors/3-0-probability-distribution-lesson-plan.html#normalizing-constant",
    "title": "Lesson Plan: Probability Distributions",
    "section": "Normalizing Constant",
    "text": "Normalizing Constant\nNormalizing Constant - is a value that ensures that a probability density function has a probability of 1. This constant could be a scalar value, equation, or function. Every probability distribution that doesn’t sum to 1 will have a normalizing constant.\nSometimes the calculation of the normalizing constant will be easy to compute. Below, we will look at this idea utilizing the Uniform Distribution. - We know that the area of this rectangle created by the Uniform distribution equals 1."
  },
  {
    "objectID": "instructors/3-0-probability-distribution-lesson-plan.html#calculation-and-practice-question-5-solution",
    "href": "instructors/3-0-probability-distribution-lesson-plan.html#calculation-and-practice-question-5-solution",
    "title": "Lesson Plan: Probability Distributions",
    "section": "Calculation and Practice: Question 5 & Solution",
    "text": "Calculation and Practice: Question 5 & Solution\nQuestion 5 Compute the normalizing constant when we have U(a,b).\n\n\n Solution: We know that Area of Rectangle = base x height\n\nArea of Rectangle = 1\nbase x height = 1\nbase = (b-a), this is substituted in the equation\n(b-a) x height = 1\nSolve this equation for the height of this rectangle.(In this case the height is the normalizing constant)\nheight = 1/(b-a)\nNormalizing constant= 1/(b-a)\n\n\n\n\n\nFigure 13: Uniform(0,1)"
  },
  {
    "objectID": "instructors/3-0-probability-distribution-lesson-plan.html#calculation-and-practice-question-6-part-a-solution",
    "href": "instructors/3-0-probability-distribution-lesson-plan.html#calculation-and-practice-question-6-part-a-solution",
    "title": "Lesson Plan: Probability Distributions",
    "section": "Calculation and Practice: Question 6 part a & Solution",
    "text": "Calculation and Practice: Question 6 part a & Solution\nQuestion 6: Given the graph below, where the random number generator will generate numbers on the following interval [3,8].\n\n\nPart a) Construct this Uniform distribution and scale both axes to ensure the area =1.\n\n\n\n\nFigure 14: Uniform(3,8)\n\n\n\n\n\nSolution - Note that the y-axis has been scaled."
  },
  {
    "objectID": "instructors/3-0-probability-distribution-lesson-plan.html#calculation-and-practice-question-6-part-b-solution",
    "href": "instructors/3-0-probability-distribution-lesson-plan.html#calculation-and-practice-question-6-part-b-solution",
    "title": "Lesson Plan: Probability Distributions",
    "section": "Calculation and Practice: Question 6 part b & Solution",
    "text": "Calculation and Practice: Question 6 part b & Solution\nPart b) What is P(X&lt;5 or X&gt;7) for the Uniform[3,8]? Show a graph and explain your answer.\n\n\n\nSolution\nWe can calculate the area of the blue sections:\nArea = (5-3)(.2) + (8-7)(.2)\nArea = .4 + .2\nArea =.6\n\n\n\n\n\nFigure 16: P(X&lt;5 or X&gt;7) on a Uniform(3,8)"
  },
  {
    "objectID": "instructors/3-0-probability-distribution-lesson-plan.html#example",
    "href": "instructors/3-0-probability-distribution-lesson-plan.html#example",
    "title": "Lesson Plan: Probability Distributions",
    "section": "Example:",
    "text": "Example:\nThe density curve for the sum X of two random numbers that are generated on the interval [0,1].\na)     What interval would be used for the density curve of X?\n\nSolution\n [0,2] - This is due to the fact that it is possible that the two numbers your are summing are both 1. 1+1=2 which would be the maximum of the interval."
  },
  {
    "objectID": "instructors/3-0-probability-distribution-lesson-plan.html#example-1",
    "href": "instructors/3-0-probability-distribution-lesson-plan.html#example-1",
    "title": "Lesson Plan: Probability Distributions",
    "section": "Example:",
    "text": "Example:\n\n\n\nFigure 17: triangle (a,b)\n\n\nProvided is a graph of the density curve of X that is a triangle distribution symmetric about one and has a height of one. Then, calculate the area. Have students draw a picture.\n Solution\n\n\n\nFigure 18: triangle (0,2)\n\n\nArea = .5(base)(height)\nArea = .5(2unit)(1unit)\nArea= 1 unit2"
  },
  {
    "objectID": "instructors/3-0-probability-distribution-lesson-plan.html#question-7",
    "href": "instructors/3-0-probability-distribution-lesson-plan.html#question-7",
    "title": "Lesson Plan: Probability Distributions",
    "section": "Question 7",
    "text": "Question 7\nGiven a triangular distribution below where the height of the triangle =2/3 units. What is P(x&lt;1)? ______ Explain your answer.\n\n\n\nFigure 19: triangle (0,3)\n\n\n Solution\nArea of the blue triangle = .5(base)(height) = .5(1)(2/3) = 1/3 unit2\nArea of the white triangle = .5(base)(height) = .5(2)(2/3) = 2/3 unit2\nTotal area of triangle = Area of blue + white triangle = (1/3) + (2/3) = 1 unit2"
  },
  {
    "objectID": "instructors/3-0-probability-distribution-lesson-plan.html#continuous-vs.-discrete-in-practice",
    "href": "instructors/3-0-probability-distribution-lesson-plan.html#continuous-vs.-discrete-in-practice",
    "title": "Lesson Plan: Probability Distributions",
    "section": "Continuous vs. Discrete in Practice",
    "text": "Continuous vs. Discrete in Practice\nSome variables are continuous but are sometimes treated in practice as though they were discrete.  One example would be the age of students in a class.  Age is often reported as a discrete value when surveyed, even though we know that age is a factor of time that is continuous.\nGraphs of the distribution of ages of statistics students are shown below.  Which graph shows age as a discrete random variable, and which shows it as a continuous random variable?  \n\n\n\n\n\nFigure 20:\n\n\n\n\n\n\nFigure 21:"
  },
  {
    "objectID": "instructors/3-0-probability-distribution-lesson-plan.html#age-problem",
    "href": "instructors/3-0-probability-distribution-lesson-plan.html#age-problem",
    "title": "Lesson Plan: Probability Distributions",
    "section": "Age Problem",
    "text": "Age Problem\nLets look at the data table for the previous graphs.  Adding the counts gives us the total number of students in the class, which is 195 students.\nTo calculate the probability of each age, divide the count of each age by the total number of students.\n2/195 = .010, 34/195 = .175, 621/95= .318, 36/195= .185, 35/195 =.179, 18/195 = .092, 6/195 = .031, 0/195=0.0, 2/195 = .010\nChecking criteria one, we see that, inclusively, all probabilities are between 0 and 1.\nCheck criteria two by adding all of the probabilities to show that they add to 1. \n2/195 + 34/195 + 62/195 + 36/195 + 35/195 + 18/195 + 6/195 + 0/195 + 2/195 = 1"
  },
  {
    "objectID": "instructors/3-0-probability-distribution-lesson-plan.html#r-activity-add-here",
    "href": "instructors/3-0-probability-distribution-lesson-plan.html#r-activity-add-here",
    "title": "Lesson Plan: Probability Distributions",
    "section": "R activity: add here",
    "text": "R activity: add here\nBelow is some code for you to try.\n\nOnce you figure out what the output is then change code So that it will plot a normal curve with a mean of 20 and a standard deviation of 5.\nNow plot the first plot with another normal curve on the same plot with a mean of 100 and standard deviation of 25. Make one of them red and one blue.\n\n\nlibrary(bayesrules)\nlibrary(tidyverse)\n\nplot_normal(20,5) \n\n\n\nplot_normal(100,25)\n\n\n\nggplot(data.frame(x = c(-200, 200)), aes(x)) +\nstat_function(fun = dnorm, args = list(mean = 20, sd = 5), col='red') +\nstat_function(fun = dnorm, args = list(mean = 100, sd = 25), col='blue')\n\n\n\n\nGraph the following distributions to notice what the change in the standard deviation does to the shape of the distribution: N(0, 1) N(0, 2) N(0, 10) What do you notice happens to the shape of the distribution as the standard deviation increases?\n\nggplot(data.frame(x = c(-20, 20)), aes(x)) +\nstat_function(fun = dnorm, args = list(mean = 0, sd = 1), col='red') +\nstat_function(fun = dnorm, args = list(mean = 0, sd = 2), col='blue') +\nstat_function(fun = dnorm, args = list(mean = 0, sd = 10), col='green') \n\n\n\n\nSolution When the mean stays constant and the standard deviation increases we see that the graphs stay centered at the same value which is 0 in this problem. The standard deviation causes the graph to flatten.\nCan you have a negative standard deviation? Try it out to check your answer. Graph to notice what the change in the Mean does to the shape of the distribution: N(0,1) N(2, 1) N(10, 1) What do you notice happens to the distributions shape as the mean increases? Can you have a negative mean? Try it to check your answer.\n\nggplot(data.frame(x = c(-4, 8)), aes(x)) +\nstat_function(fun = dt, args = list(df = 10), col='red') +\nstat_function(fun = dt, args = list(df = 5,ncp = 2), col='blue')\n\n\n\n\n\nggplot(data.frame(x = c(0, 6)), aes(x)) +\nstat_function(fun = dunif, args = list(min = .5, max = 1), col='red') +\nstat_function(fun = dunif, args = list(min = 2, max = 5), col='blue')\n\n\n\n\n\nggplot(data.frame(x = c(0, 6)), aes(x)) +\nstat_function(fun = dbeta, args = list(shape1 = 2, shape2 = 1), col='red') +\nstat_function(fun = dbeta, args = list(shape1 = 2, shape2 = 2), col='blue')"
  },
  {
    "objectID": "instructors/3-0-probability-distribution-lesson-plan.html#discussion-and-wrap-up",
    "href": "instructors/3-0-probability-distribution-lesson-plan.html#discussion-and-wrap-up",
    "title": "Lesson Plan: Probability Distributions",
    "section": "Discussion and Wrap-Up:",
    "text": "Discussion and Wrap-Up:\n\nFacilitate a class discussion to review the example problems, reinforce key concepts, and answer any questions the students have."
  },
  {
    "objectID": "instructors/3-0-probability-distribution-lesson-plan.html#homework",
    "href": "instructors/3-0-probability-distribution-lesson-plan.html#homework",
    "title": "Lesson Plan: Probability Distributions",
    "section": "Homework:",
    "text": "Homework:\n\nHave them work on the assessment questions in this plan."
  },
  {
    "objectID": "instructors/3-0-probability-distribution-lesson-plan.html#formative-assessment",
    "href": "instructors/3-0-probability-distribution-lesson-plan.html#formative-assessment",
    "title": "Lesson Plan: Probability Distributions",
    "section": "Formative Assessment:",
    "text": "Formative Assessment:\n\nEvaluate students based on their participation in discussions, their ability to solve example problems, and their performance on the assigned homework."
  },
  {
    "objectID": "instructors/3-0-probability-distribution-lesson-plan.html#assessment",
    "href": "instructors/3-0-probability-distribution-lesson-plan.html#assessment",
    "title": "Lesson Plan: Probability Distributions",
    "section": "Assessment:",
    "text": "Assessment:\n\nIn forensic accounting, faked numbers in tax returns, invoices, expense account claims, and other financial records display patterns that aren’t present in legitimate records. Some of these “fakes” are easy to spot, for instance, if there are many rounded numbers. But Benford’s law tells that the first digits of numbers in legitimate records often follow the following distribution:\n\n\nWhat type of probability distribution (discrete or continuous) is represented here in this table? \n\n Solution:  A Discrete Probability Distribution \n\nConsider these events and calculate their probabilities of occuring using the table above:\nA = {The first digit is a 5.}\nB = {The first digit is a 3 or less.}\nC = {The first digit is greater than 7.}\n\n Solutions\ni.     P(A) = .079\nii.     P(B) =.301+.176+.125 =.602\niii.     P(C) =.051+.046 = .097\n\n\nMake a graph of this distribution.\n\n Solution \n\n\n\n\n\nGiven a probability distribution in the table below of the number of significant volcanic eruptions between 2010 and 2023 worldwide.\n\n\nIs this a discrete or continuous probability distribution?\nExplain why you made this choice.\nGraph this distribution.\n\n\n\n\n\nTable 4: The Number of Signigicant Volcanic Eruptions in the World from 2010-2022\n\n\n\n\n Solutions\nSome students may say discrete, and some say continuous. This would mean students may display a bar graph (Figure 22) or a histogram(Figure 23). \nTypically I would say this continuous because the one variable, year, is a unit of time. I would have used a histogram to plot this data as seen in Figure 23.  \n\n\n\nFigure 22\n\n\n\n\n\nFigure 23"
  },
  {
    "objectID": "instructors/3-0-probability-distribution-lesson-plan.html#conclusion",
    "href": "instructors/3-0-probability-distribution-lesson-plan.html#conclusion",
    "title": "Lesson Plan: Probability Distributions",
    "section": "Conclusion:",
    "text": "Conclusion:\n\nHave the students summarize what they learned today.  Make sure to do some re-voicing of what students are saying. Also, ask students to explain what one of their peers said.\nIf you have time, give the students an Exit Ticket and include some questions that check for understanding. You may ask students to tell you what they are confident they understand and then list a topic or concept they are still grappling with to understand. This will help you plan for any necessary review at the beginning of the next class."
  },
  {
    "objectID": "instructors/1-0-probability-1-lesson-plan.html",
    "href": "instructors/1-0-probability-1-lesson-plan.html",
    "title": "Review of Basic Probability - Part 1",
    "section": "",
    "text": "Objective:\n\nBy the end of this lesson, students will have reviewed the following topics:\n\nDefinition of events\nDefinition of sample space\nDefinition of probability\nLaw of Large Numbers\nAddition Rule for mutually exclusive events\nGeneral Addition Rule\nComplement Rule\n\n\nDuration:\n\n75 minutes\n\nMaterials:\n\nHandouts with exercises and problems related to basic probability\nComputer, projector, and screen\n\nIntroduction:\n\nExamples that can be used to jump start topic:\n\nbirthday problem\nMonty Hall problem\ncasino games\n\n\nIntroduce the lesson’s topic:\n\nToday we will review basic probability rules.\n\nHistorical Context:\n\n1560s: Cardano wrote Liber de ludo aleae, the first known systematic treatment of probability, and as the result of a gambling addiction.\n1654: Fermat and Pascal worked on the foundation of probability theory through correspondence.\n1812 and 1814: Laplace published Théorie analytique des probabilités and Essai philosophique sur les probabilités, outlining many basic and fundamental results in statistics.\n\nMain Content:\n\nReview of Events, Sample Spaces, and Probability:\n\nProbability: a number between 0 and 1 that measures the uncertainty of a particular event.\n\n\\(p = 0 \\to\\) event will never happen.\n\\(p = 1 \\to\\) event will definitely happen.\n\nIntuitive probability:\n\nWhat is the probability of being pulled over while speeding in Gulf Breeze?\nWhat is the probability of seeing a penguin walk around UWF?\nWhat is the probability of seeing an armadillo walk around UWF?\n\nEvents: an outcome (or collection of outcomes) in a statistical experiment\n\nEvents can be defined and described in three ways (1.7 in Albert and Hu):\n\n\\(A \\cap B\\) is the intersection between \\(A\\) and \\(B\\); it is the event that both \\(A\\) and \\(B\\) occur.\n\\(A \\cup B\\) is the union between \\(A\\) and \\(B\\); it is the event that either \\(A\\) or \\(B\\) occur.\n\\(A^c\\) is the complement of \\(A\\); it is the event that \\(A\\) does not occur.\n\n\nSample spaces: all possible outcomes of a statistical experiment\n\nAddition Rules\n\nMutually exclusive events: events that have no outcomes in common; also known as disjoint\n\nVenn Diagram examples\n\nAddition Rule for Mutually Exclusive Events: if \\(A\\) and \\(B\\) are mutually exclusive events, then \\[P(A \\cup B) = P(A) + P(B)\\]\n\nExamples: 1 instructor works through, 2 students work through\n\nGeneral Addition Rule: regardless of \\(A\\) and \\(B\\) being mutually exclusive events, \\[P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\]\n\nExamples: 1 instructor works through, 2 students work through\n\nComplements: if \\(S\\) is the sample space, then the complement of event \\(A\\), denoted by \\(A^c\\) is all outcomes in \\(S\\) that are not outcomes in event \\(A\\).\nComplement Rule: consider any event \\(A\\) and its complement, \\(A^c\\). From probability rules, we know \\[P(A^c) = 1 - P(A)\\]\n\nExamples: 1 the instructor works through, 2 students work through\n\n\n\nCalculation and Practice:\n\nExamples for events and sample spaces:\n\nExample 1:\nExample 2:\nExample 3:\n\nExamples for the Addition Rule for Mutually Exclusive Events:\n\nExample 1\nExample 2\nExample 3\n\nExamples for the General Addition Rule:\n\nExample 1\nExample 2\nExample 3\n\n\nDiscussion and Wrap-Up:\n\nFacilitate a class discussion to review the example problems, reinforce key concepts, and answer any questions the students have.\n\nHomework:\n\nAssign additional problems to practice the basic probability rules.\n\nFormative Assessment:\n\nEvaluate students based on their participation in discussions, their ability to solve example problems, and their performance on the assigned homework.\n\nConclusion:\n\nEmphasize these are building blocks for the next lesson and long term understanding probability."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Bayesian Thinking",
    "section": "",
    "text": "In Tier 2 of the Bayes BATS program, we created lesson plans for instructors appropriate for the advanced undergraduate or applied graduate level. This website outlines a Bayesian course of that nature. As collaboration continues, skeleton notes for students to participate in the programming aspect of the course will be added. Please keep checking back as this website will continually be updated as the instructors teach Bayesian courses and receive feedback from others. (Last updated January 15, 2024.)"
  },
  {
    "objectID": "about.html#about-this-site",
    "href": "about.html#about-this-site",
    "title": "Bayesian Thinking",
    "section": "",
    "text": "In Tier 2 of the Bayes BATS program, we created lesson plans for instructors appropriate for the advanced undergraduate or applied graduate level. This website outlines a Bayesian course of that nature. As collaboration continues, skeleton notes for students to participate in the programming aspect of the course will be added. Please keep checking back as this website will continually be updated as the instructors teach Bayesian courses and receive feedback from others. (Last updated January 15, 2024.)"
  },
  {
    "objectID": "about.html#about-the-authors",
    "href": "about.html#about-the-authors",
    "title": "Bayesian Thinking",
    "section": "About the Authors",
    "text": "About the Authors\nDr. Abraham Ayebo is an Associate Professor in the Center for Learning Innovation at the University of Minnesota Rochester. He can be reached at aayebo at r dot umn dot edu.\nDr. Samantha Seals is an Associate Professor of Statistics in the Department of Mathematics and Statistics at the University of West Florida. Dr. Seals teaches upper-division and graduate statistics, biostatistics, and data science courses. She can be reached at sseals at uwf dot edu.\nDr. Toni Sorrell is an Associate Professor of Math Education in the Department of Mathematics and Computer Science Department at Longwood University. She can be reached at sorrelltp at longwood dot edu."
  },
  {
    "objectID": "about.html#courses-using-this-content",
    "href": "about.html#courses-using-this-content",
    "title": "Bayesian Thinking",
    "section": "Courses Using This Content",
    "text": "Courses Using This Content\n\nApplied Bayesian Analysis Spring 2024 (Dr. Seals)\nBayesian Statistics Spring 2024 (Dr. Ayebo)"
  },
  {
    "objectID": "about.html#funding",
    "href": "about.html#funding",
    "title": "Bayesian Thinking",
    "section": "Funding",
    "text": "Funding\nThis work is the result of the authors’ participation in the BATS program, which is supported by NSF IUSE: EHR program with award numbers 2215879, 2215920, and 2215709."
  },
  {
    "objectID": "examples/8-1-normal-normal-in-class-examples.html",
    "href": "examples/8-1-normal-normal-in-class-examples.html",
    "title": "Examples for Normal-Normal",
    "section": "",
    "text": "library(bayesrules)\nlibrary(tidyverse)"
  },
  {
    "objectID": "examples/8-1-normal-normal-in-class-examples.html#the-normal-model",
    "href": "examples/8-1-normal-normal-in-class-examples.html#the-normal-model",
    "title": "Examples for Normal-Normal",
    "section": "The Normal Model",
    "text": "The Normal Model\nRecall the normal model,\n\\[f(y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left\\{-\\frac{\\left(y - \\mu\\right)^2}{2\\sigma^2} \\right\\}\\]\nwhere \\(y\\) is a continuous random variable that can take any value in \\(\\mathbb{R}\\); i.e., \\(Y \\in \\left(-\\infty, \\infty\\right)\\).\nSuppose we have \\(n\\) observations, then the joint distribution is given by\n\\[f(\\overset{\\to}{y}|\\mu) = \\prod_{i=1}^n f(y_i|\\mu) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left\\{-\\frac{\\left(y_i - \\mu\\right)^2}{2\\sigma^2} \\right\\}\\]\nThen the likelihood is given by,\n\\[\n\\begin{align*}\nL(\\mu| \\overset{\\to}{y}) &\\propto \\prod_{i=1}^n \\exp\\left\\{-\\frac{\\left(y_i - \\mu\\right)^2}{2\\sigma^2} \\right\\} = \\exp\\left\\{-\\frac{\\sum_{i=1}^n \\left(y_i - \\mu\\right)^2}{2\\sigma^2} \\right\\} \\\\\n&\\propto \\exp\\left\\{-\\frac{\\left(\\bar{y} - \\mu\\right)^2}{2\\sigma^2/n} \\right\\}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "examples/8-1-normal-normal-in-class-examples.html#the-normal-normal-model",
    "href": "examples/8-1-normal-normal-in-class-examples.html#the-normal-normal-model",
    "title": "Examples for Normal-Normal",
    "section": "The Normal-Normal Model",
    "text": "The Normal-Normal Model\n\nLet \\(\\mu \\in (-\\infty, \\infty)\\) be an unknown mean parameter and \\((Y_1, Y_2, ..., Y_n)\\) be an independent \\(N(\\mu, \\sigma^2)\\), where \\(\\sigma\\) is assumed to be known.\nThe Normal-Normal Bayesian model has Normal distributions for both prior and data. The Normal prior is on the unknown mean, \\(\\mu\\).\n\n\\[Y_i | \\mu \\overset{\\text{ind}}{\\sim} N(\\mu, \\sigma^2)\\]\n\\[\\mu \\sim N(\\theta, \\tau^2)\\]\n\nWhen we have data \\(\\overset{\\to}{y} = (y_1, ..., y_n)\\) with mean \\(\\bar{y}\\), the posterior distribution for \\(\\mu\\) is also Normal with updated parameters,\n\n\\[\\mu|\\overset{\\to}{y} \\sim N\\left( \\theta \\frac{\\sigma^2}{n\\tau^2 + \\sigma^2} + \\bar{y} \\frac{n\\tau^2}{n\\tau^2+\\sigma^2}, \\frac{\\tau^2 \\sigma^2}{n\\tau^2 + \\sigma^2} \\right)\\]\n\nExample 1aExample 1b\n\n\n\nLet’s apply this! Consider analysis of \\(\\mu\\), the average hippocampal volume among people that have a history of concussions.\nFirst, let’s derive the prior distribution. From the textbook, we can reasonably assume that the hippocampal volumes of our \\(n=25\\) subjects, \\(\\overset{\\to}{Y_i} = (y_1, ..., y_n)\\), are independent and normally distributed around a mean volume, \\(\\mu\\), with standard deviation \\(\\sigma\\).\nWe don’t have prior information about this very specific group, however, Wikipedia tells us that among the general population of human adults, each half of the hippocampus has a volume between 3.0 and 3.5 cm3 \\(\\to\\) the total hippocampal volume of both sides of the brain is between 6 and 7 cm3.\nBecause \\(\\sigma\\) is a nuisance parameter, we’ll assume that the standard deviation is known to be \\(\\sigma=0.5\\) cm3.\nWhat is the prior distribution in mathematical notation?\nGraph the prior distribution, below:\n\n\nImport the FootballBrain data from the Lock5Data package.\n\n\nOnly include those that have a history of concussions (fb_concuss).\n\n\nWhat is the average hippocampal volume (volume) of those that have a history of concussions?\n\n\nConstruct the density plot for the hippocampal volume (volume)\n\n\nDoes the normal distribution seem reasonable to use here?\nPlot the Normal-Normal model using the plot_normal_normal() function from the bayesrules package.\n\n\nRun the summarize_normal_normal() function from the bayesrules package.\n\n\n\n\nLet’s now explore those that did not have prior concussions.\nRe-import the FootballBrain data from the Lock5Data package.\n\n\nOnly include those that do not have a history of concussions (fb_no_concuss).\n\n\nFind the average mean hippocampal volume and sample size of the control subjects who have not been diagnosed with a concussion.\n\n\nFind the posterior model of \\(\\mu\\) using summarize_normal_normal() from the bayesrules package.\n\n\nPlot the Normal-Normal model using the plot_normal_normal() function from the bayesrules package.\n\n\n\n\n\nExample 2aExample 2bExample 2cExample 2d\n\n\n\nSuppose you just bought stock in Mojo’s Bakery. Let \\(\\mu\\) be a random variable that represents the average dollar amount that your stock in Mojo’s Bakery goes up or down in a one-day period.\nYou believe that \\(\\mu=8.7\\) dollars with a standard deviation of \\(2.9\\) dollars is a reasonable starting point.\nPlot the appropriate Normal prior model for \\(\\mu\\).\n\n\nDoes it seem plausible that the stock would increase by an average of 9.1 dollars in one day?\nDoes it seem plausible that the stock would increase by an average of 5 dollars in one day?\nWhat is the prior probability that, on average, the stock price decreases?\n\n\nWhat is the prior probability that, on average, the stock price increases by more than 9.5 dollars per day?\n\n\n\n\nSuppose we assume that the daily changes in Mojo’s Bakery stock are normally distributed around an unknown mean of \\(\\mu\\) with a known standard deviation of \\(\\sigma=2\\) dollars.\nOn a random sample of 4 days, you observe changes in stock value of –0.6, 1.8, 3.9, and –4.1 dollars.\nPlot the corresponding likelihood function of \\(\\mu\\).\n\n\nPlot the Normal-Normal model using the plot_normal_normal() function from the bayesrules package.\n\n\nUse summarize_normal_normal() from the bayesrules package to calculate descriptive statistics for the prior and posterior models.\n\n\nWhat is the posterior probability that, on average, the stock price goes down?\n\n\nWhat is the posterior probability that, on average, the stock price goes up by more than 9.5 dollars per day?\n\n\n\n\nSuppose we assume that the daily changes in Mojo’s Bakery stock are normally distributed around an unknown mean of \\(\\mu\\) with a known standard deviation of \\(\\sigma=1\\) dollars.\nOn a random sample of 4 days, you observe changes in stock value of –0.6, 1.8, 3.9, and –4.1 dollars.\nPlot the corresponding likelihood function of \\(\\mu\\).\n\n\nPlot the Normal-Normal model using the plot_normal_normal() function from the bayesrules package.\n\n\nUse summarize_normal_normal() from the bayesrules package to calculate descriptive statistics for the prior and posterior models.\n\n\nWhat is the posterior probability that, on average, the stock price goes down?\n\n\nWhat is the posterior probability that, on average, the stock price goes up by more than 9.5 dollars per day?\n\n\n\n\nSuppose we assume that the daily changes in Mojo’s Bakery stock are normally distributed around an unknown mean of \\(\\mu\\) with a known standard deviation of \\(\\sigma=3\\) dollars.\nOn a random sample of 4 days, you observe changes in stock value of –0.6, 1.8, 3.9, and –4.1 dollars.\nPlot the corresponding likelihood function of \\(\\mu\\).\n\n\nPlot the Normal-Normal model using the plot_normal_normal() function from the bayesrules package.\n\n\nUse summarize_normal_normal() from the bayesrules package to calculate descriptive statistics for the prior and posterior models.\n\n\nWhat is the posterior probability that, on average, the stock price goes down?\n\n\nWhat is the posterior probability that, on average, the stock price goes up by more than 9.5 dollars per day?"
  },
  {
    "objectID": "examples/1-1-probability-in-class-examples.html",
    "href": "examples/1-1-probability-in-class-examples.html",
    "title": "Examples for Review of Basic Probability - Part 1",
    "section": "",
    "text": "Events and Sample Spaces\n\nExample 1Example 2Example 3\n\n\n\nSuppose we roll two fair, indistinguishable, dice.\n\nWhat is the sample space?\nDoes order matter for the experiment? Why or why not?\nLet event \\(A\\) be rolling doubles. What are the outcomes that belong to event \\(A\\)?\nLet event \\(B\\) be rolling at least one odd number. What are the outcomes that belong to event \\(B\\)?\n\nTable of possible outcomes when rolling two fair, indistinguishable dice:\n\n\n\n\n\n\n\n\n(1, 1)\n\n\n(1, 2)\n\n\n(1, 3)\n\n\n(1, 4)\n\n\n(1, 5)\n\n\n(1, 6)\n\n\n\n\n\n\n(2, 1)\n\n\n(2, 2)\n\n\n(2, 3)\n\n\n(2, 4)\n\n\n(2, 5)\n\n\n(2, 6)\n\n\n\n\n(3, 1)\n\n\n(3, 2)\n\n\n(3, 3)\n\n\n(3, 4)\n\n\n(3, 5)\n\n\n(3, 6)\n\n\n\n\n(4, 1)\n\n\n(4, 2)\n\n\n(4, 3)\n\n\n(4, 4)\n\n\n(4, 5)\n\n\n(4, 6)\n\n\n\n\n(5, 1)\n\n\n(5, 2)\n\n\n(5, 3)\n\n\n(5, 4)\n\n\n(5, 5)\n\n\n(5, 6)\n\n\n\n\n(6, 1)\n\n\n(6, 2)\n\n\n(6, 3)\n\n\n(6, 4)\n\n\n(6, 5)\n\n\n(6, 6)\n\n\n\n\n\n\n\n\n\nConsider the word TENNESSEE. Suppose we were to randomly select a letter.\n\nWhat is the sample space?\nWhat is \\(P(T)\\)?\nWhat is \\(P(E)\\)?\nWhat is \\(P(N)\\)?\nWhat is \\(P(S)\\)?\n\n\n\n\n\nSet up: split class into various groups.\nSuppose we roll two indistinguishable fair dice. We are interested in the sum of the numbers on the two dice.\n\nWhat does the sample space become?\nDoes order matter for the experiment? Why or why not?\nLet event \\(A\\) be rolling an even sum. What are the outcomes that belong to event \\(A\\)?\n\nWhat is \\(P(A)\\)?\n\nLet event \\(B\\) be rolling a sum that is a prime number. What are the outcomes that belong to event \\(B\\)?\n\nWhat is \\(P(B)\\)?\n\nAssign each group an event and have them find the probabilities.\n\n\\(C\\): roll a sum that is an odd sum (bonus: complement rule early :))\n\\(D\\): roll a sum that is (strictly) less than 5.\n\\(E\\): roll an even sum that is 9 or greater.\netc.\n\nHave one member of each group present the set of outcomes belonging to the event.\nHave another member present how to find the probability.\n\nTable of possible outcomes:\n\n\n\n\n\n\n\n\n(1, 1) = 2\n\n\n(1, 2) = 3\n\n\n(1, 3) = 4\n\n\n(1, 4) = 5\n\n\n(1, 5) = 6\n\n\n(1, 6) = 7\n\n\n\n\n\n\n(2, 1) = 3\n\n\n(2, 2) = 4\n\n\n(2, 3) = 5\n\n\n(2, 4) = 6\n\n\n(2, 5) = 7\n\n\n(2, 6) = 8\n\n\n\n\n(3, 1) = 4\n\n\n(3, 2) = 5\n\n\n(3, 3) = 6\n\n\n(3, 4) = 7\n\n\n(3, 5) = 8\n\n\n(3, 6) = 9\n\n\n\n\n(4, 1) = 5\n\n\n(4, 2) = 6\n\n\n(4, 3) = 7\n\n\n(4, 4) = 8\n\n\n(4, 5) = 9\n\n\n(4, 6) = 10\n\n\n\n\n(5, 1) = 6\n\n\n(5, 2) = 7\n\n\n(5, 3) = 8\n\n\n(5, 4) = 9\n\n\n(5, 5) = 10\n\n\n(5, 6) = 11\n\n\n\n\n(6, 1) = 7\n\n\n(6, 2) = 8\n\n\n(6, 3) = 9\n\n\n(6, 4) = 10\n\n\n(6, 5) = 11\n\n\n(6, 6) = 12\n\n\n\n\n\n\n\n\n\n\n\nAddition Rule for Mutually Exclusive Events\n\nExample 1Example 2Example 3\n\n\n\nSuppose a single card is drawn from a standard 52-card deck.\n\nWhat is the sample space?\n\nSuppose event \\(A\\) is drawing a face card (J, Q, K).\n\nWhat is \\(P(A)\\)?\n\nSuppose event \\(B\\) is drawing a black, even card.\n\nWhat is \\(P(B)\\)?\n\nAre \\(A\\) and \\(B\\) mutually exclusive events? Why or why not?\n\nWhat is \\(P(A \\cup B)\\)?\n\n\nTable of card deck:\n\n\n\n\n\n\n\nClubs:\n\n\nA\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\nJ\n\n\nQ\n\n\nK\n\n\n\n\n\n\nSpades:\n\n\nA\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\nJ\n\n\nQ\n\n\nK\n\n\n\n\nDiamonds:\n\n\nA\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\nJ\n\n\nQ\n\n\nK\n\n\n\n\nHearts:\n\n\nA\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\nJ\n\n\nQ\n\n\nK\n\n\n\n\n\n\n\n\n\nSuppose we toss three coins.\n\nWhat is the sample space?\n\nLet event \\(A\\) be flipping at least two tails.\n\nWhat is \\(P(A)\\)?\n\nLet event \\(B\\) be flipping no tails.\n\nWhat is \\(P(B)\\)?\n\nLet event \\(C\\) be flipping no heads.\n\nWhat is \\(P(C)\\)?\n\nAre the following events mutually exclusive? Why or why not?\n\n\\(A\\) and \\(B\\)\n\\(A\\) and \\(C\\)\n\\(B\\) and \\(C\\)\n\n\nSample Space:\n\n\n\n\n\n\n\nHHH\n\n\n\n\n\n\n\n\n\n\nHHT\n\n\nHTH\n\n\nTHH\n\n\n\n\nHTT\n\n\nTHT\n\n\nTTH\n\n\n\n\nTTT\n\n\n\n\n\n\n\n\n\n\n\n\n\nSet up: split class into various groups.\nSuppose we are rolling two dice: one red, one blue.\nAssign each group an event and have them find the probabilities:\n\n\\(A\\): Rolling a red 2.\n\\(B\\): Rolling a blue 5.\n\\(C\\): Rolling a red odd.\n\\(D\\): Rolling a sum that is odd.\n\\(E\\): Rolling a sum that is even.\netc.\n\nHave groups determine which other groups they are mutually exclusive with.\nHave groups find other groups they are mutually exclusive with and find \\(P(E_1 \\cup E_2)\\).\n\n\n\n\n\n\nExamples for the General Addition Rule\n\nExample 1Example 2Example 3Example 4\n\n\n\nThe probability of a teenager owning a Playstation is 0.31, of owning a Switch is 0.56 and of owning both is 0.17.\n\nWhat are the events that are defined by the problem?\n\nIf a teenager is chosen at random, what is the probability that the teenager owns a Playstation or Switch?\n\nWhat is \\(P(\\text{Playstation})\\)?\nWhat is \\(P(\\text{Switch})\\)?\nWhat is \\(P(\\text{Playstation} \\cap \\text{Switch})\\)?\nWhat is \\(P(\\text{Playstation} \\cup \\text{Switch})\\)?\n\nSuggestion: Venn Diagram\n\n\n\n\nThere are 100 students taking either STA4173 (Biostatistics) or STA4231 (Statistics for Data Science I). 80 students are taking Biostatistics and 30 students are taking Statistics for Data Science I.\n\nWhat is \\(P(\\text{Biostatistics})\\)?\nWhat is \\(P(\\text{Statistics for Data Science I})\\)?\nWhat is \\(P(\\text{Biostatistics} \\cap \\text{Statistics for Data Science I})\\)?\nWhat is \\(P(\\text{Biostatistics} \\cup \\text{Statistics for Data Science I})\\)?\n\nSuggestion: Venn Diagram\n\n\n\n\nSuppose a single card is drawn from a standard 52-card deck.\n\nWhat is the sample space?\n\nSuppose event \\(A\\) is drawing a face card (J, Q, K).\n\nWhat is \\(P(A)\\)?\n\nSuppose event \\(B\\) is drawing a red card.\n\nWhat is \\(P(B)\\)?\n\nAre \\(A\\) and \\(B\\) mutually exclusive events? Why or why not?\nWhat is \\(P(A \\cup B)\\)?\n\nTable of card deck:\n\n\n\n\n\n\n\nClubs:\n\n\nA\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\nJ\n\n\nQ\n\n\nK\n\n\n\n\n\n\nSpades:\n\n\nA\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\nJ\n\n\nQ\n\n\nK\n\n\n\n\nDiamonds:\n\n\nA\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\nJ\n\n\nQ\n\n\nK\n\n\n\n\nHearts:\n\n\nA\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\nJ\n\n\nQ\n\n\nK\n\n\n\n\n\n\n\n\n\nSet up: split class into various groups.\nSuppose two cards are drawn without replacement from a standard 52-card deck.\nAssign each group an event and have them find the corresponding probabilities.\n\n\\(A\\): drawing two even cards\n\\(B\\): drawing two face cards\n\\(C\\): drawing a red 2 and black 3\netc.\n\nHave one student from each group present their probabilities.\nPair groups together and ask them to find \\(P(E_1 \\cup E_2)\\).\nHave one student from each paired group present their solution.\n\nTable of card deck:\n\n\n\n\n\n\n\nClubs:\n\n\nA\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\nJ\n\n\nQ\n\n\nK\n\n\n\n\n\n\nSpades:\n\n\nA\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\nJ\n\n\nQ\n\n\nK\n\n\n\n\nDiamonds:\n\n\nA\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\nJ\n\n\nQ\n\n\nK\n\n\n\n\nHearts:\n\n\nA\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\nJ\n\n\nQ\n\n\nK"
  }
]